{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-09T16:10:21.944302Z","iopub.execute_input":"2023-10-09T16:10:21.944796Z","iopub.status.idle":"2023-10-09T16:10:26.390054Z","shell.execute_reply.started":"2023-10-09T16:10:21.944762Z","shell.execute_reply":"2023-10-09T16:10:26.388949Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.8/site-packages (4.34.0)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/site-packages (from transformers) (4.65.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.8/site-packages (from transformers) (0.17.3)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/site-packages (from transformers) (6.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from transformers) (23.1)\nRequirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.8/site-packages (from transformers) (0.14.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/site-packages (from transformers) (2023.10.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.8/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.8/site-packages (from transformers) (0.4.0)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.9.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (1.26.16)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (2023.5.7)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom torch.utils.data import DataLoader, Dataset\n","metadata":{"execution":{"iopub.status.busy":"2023-10-09T16:10:26.391758Z","iopub.execute_input":"2023-10-09T16:10:26.392129Z","iopub.status.idle":"2023-10-09T16:10:26.396739Z","shell.execute_reply.started":"2023-10-09T16:10:26.392101Z","shell.execute_reply":"2023-10-09T16:10:26.396039Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"!pip install tokenizers sentencepiece torch transformers\n","metadata":{"execution":{"iopub.status.busy":"2023-10-09T16:10:26.397518Z","iopub.execute_input":"2023-10-09T16:10:26.397729Z","iopub.status.idle":"2023-10-09T16:10:30.832928Z","shell.execute_reply.started":"2023-10-09T16:10:26.397709Z","shell.execute_reply":"2023-10-09T16:10:30.831722Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tokenizers in /usr/local/lib/python3.8/site-packages (0.14.1)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.8/site-packages (0.1.99)\nRequirement already satisfied: torch in /usr/local/lib/python3.8/site-packages (2.0.0)\nRequirement already satisfied: transformers in /usr/local/lib/python3.8/site-packages (4.34.0)\nRequirement already satisfied: huggingface_hub<0.18,>=0.16.4 in /usr/local/lib/python3.8/site-packages (from tokenizers) (0.17.3)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/site-packages (from torch) (4.7.1)\nRequirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.8/site-packages (from torch) (2.14.3)\nRequirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.8/site-packages (from torch) (11.4.0.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.8/site-packages (from torch) (3.12.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.8/site-packages (from torch) (3.1.2)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/site-packages (from torch) (11.10.3.66)\nRequirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.8/site-packages (from torch) (2.0.0)\nRequirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.8/site-packages (from torch) (11.7.101)\nRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.8/site-packages (from torch) (10.9.0.58)\nRequirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.8/site-packages (from torch) (11.7.4.91)\nRequirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.8/site-packages (from torch) (11.7.91)\nRequirement already satisfied: sympy in /usr/local/lib/python3.8/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /usr/local/lib/python3.8/site-packages (from torch) (3.1)\nRequirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.8/site-packages (from torch) (10.2.10.91)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/site-packages (from torch) (11.7.99)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/site-packages (from torch) (11.7.99)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/site-packages (from torch) (8.5.0.96)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (57.5.0)\nRequirement already satisfied: wheel in /usr/local/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\nRequirement already satisfied: cmake in /usr/local/lib/python3.8/site-packages (from triton==2.0.0->torch) (3.26.4)\nRequirement already satisfied: lit in /usr/local/lib/python3.8/site-packages (from triton==2.0.0->torch) (16.0.6)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from transformers) (23.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.8/site-packages (from transformers) (0.4.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/site-packages (from transformers) (4.65.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/site-packages (from transformers) (2023.10.3)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/site-packages (from transformers) (6.0)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.8/site-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers) (2023.9.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (2023.5.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (1.26.16)\nRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install sentencepiece\n","metadata":{"execution":{"iopub.status.busy":"2023-10-09T16:10:30.836394Z","iopub.execute_input":"2023-10-09T16:10:30.836760Z","iopub.status.idle":"2023-10-09T16:10:35.144900Z","shell.execute_reply.started":"2023-10-09T16:10:30.836721Z","shell.execute_reply":"2023-10-09T16:10:35.143841Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/site-packages (0.1.99)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"Open-Orca/Mistral-7B-OpenOrca\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Open-Orca/Mistral-7B-OpenOrca\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade transformers\n","metadata":{"execution":{"iopub.status.busy":"2023-10-09T16:11:34.331858Z","iopub.execute_input":"2023-10-09T16:11:34.332292Z","iopub.status.idle":"2023-10-09T16:11:38.937950Z","shell.execute_reply.started":"2023-10-09T16:11:34.332260Z","shell.execute_reply":"2023-10-09T16:11:38.936814Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.8/site-packages (4.34.0)\nRequirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.8/site-packages (from transformers) (0.14.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.8/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/site-packages (from transformers) (4.65.0)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.8/site-packages (from transformers) (0.4.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/site-packages (from transformers) (2023.10.3)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.8/site-packages (from transformers) (0.17.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from transformers) (23.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/site-packages (from transformers) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.9.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (1.26.16)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (3.2.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (2023.5.7)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (3.4)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import set_seed\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\nset_seed(42)\n\n# Load pre-trained model and tokenizer\nmodel_name = \"Open-Orca/Mistral-7B-OpenOrca\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"Open-Orca/Mistral-7B-OpenOrca\")","metadata":{"execution":{"iopub.status.busy":"2023-10-09T16:15:09.581521Z","iopub.execute_input":"2023-10-09T16:15:09.581868Z","iopub.status.idle":"2023-10-09T16:20:48.834288Z","shell.execute_reply.started":"2023-10-09T16:15:09.581837Z","shell.execute_reply":"2023-10-09T16:20:48.832888Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"Downloading (…)lve/main/config.json: 100%|██████████| 623/623 [00:00<00:00, 72.9kB/s]\nDownloading (…)model.bin.index.json: 100%|██████████| 23.9k/23.9k [00:00<00:00, 10.3MB/s]\nDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\nDownloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   0%|          | 10.5M/9.94G [00:00<02:24, 68.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   0%|          | 21.0M/9.94G [00:00<04:50, 34.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   0%|          | 41.9M/9.94G [00:00<03:23, 48.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   1%|          | 52.4M/9.94G [00:01<03:59, 41.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   1%|          | 73.4M/9.94G [00:01<02:53, 56.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   1%|          | 83.9M/9.94G [00:01<03:26, 47.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   1%|          | 94.4M/9.94G [00:02<03:42, 44.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   1%|          | 105M/9.94G [00:02<03:56, 41.6MB/s] \u001b[A\nDownloading (…)l-00001-of-00002.bin:   1%|          | 115M/9.94G [00:02<04:42, 34.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   1%|▏         | 136M/9.94G [00:03<03:49, 42.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   1%|▏         | 147M/9.94G [00:03<04:00, 40.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   2%|▏         | 168M/9.94G [00:03<03:16, 49.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   2%|▏         | 178M/9.94G [00:04<04:12, 38.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   2%|▏         | 199M/9.94G [00:04<04:07, 39.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   2%|▏         | 210M/9.94G [00:04<04:11, 38.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   2%|▏         | 231M/9.94G [00:05<03:32, 45.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   2%|▏         | 241M/9.94G [00:05<04:10, 38.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   3%|▎         | 262M/9.94G [00:06<03:54, 41.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   3%|▎         | 273M/9.94G [00:06<04:10, 38.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   3%|▎         | 294M/9.94G [00:07<04:08, 38.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   3%|▎         | 304M/9.94G [00:07<04:19, 37.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   3%|▎         | 325M/9.94G [00:07<03:50, 41.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   3%|▎         | 336M/9.94G [00:08<03:50, 41.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   3%|▎         | 346M/9.94G [00:08<03:38, 43.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   4%|▎         | 357M/9.94G [00:08<03:34, 44.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   4%|▍         | 377M/9.94G [00:08<02:58, 53.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   4%|▍         | 388M/9.94G [00:09<03:42, 42.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   4%|▍         | 398M/9.94G [00:09<03:36, 44.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   4%|▍         | 409M/9.94G [00:09<03:32, 44.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   4%|▍         | 419M/9.94G [00:09<04:08, 38.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   4%|▍         | 440M/9.94G [00:10<03:12, 49.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   5%|▍         | 451M/9.94G [00:10<03:39, 43.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   5%|▍         | 472M/9.94G [00:10<03:23, 46.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   5%|▍         | 482M/9.94G [00:11<03:29, 45.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   5%|▍         | 493M/9.94G [00:11<03:02, 51.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   5%|▌         | 503M/9.94G [00:11<02:56, 53.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   5%|▌         | 514M/9.94G [00:11<03:13, 48.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   5%|▌         | 535M/9.94G [00:12<03:15, 48.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   5%|▌         | 545M/9.94G [00:12<03:14, 48.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   6%|▌         | 566M/9.94G [00:12<03:16, 47.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   6%|▌         | 577M/9.94G [00:13<03:20, 46.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   6%|▌         | 598M/9.94G [00:13<03:10, 49.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   6%|▌         | 608M/9.94G [00:13<03:47, 41.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   6%|▋         | 629M/9.94G [00:14<02:57, 52.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   6%|▋         | 640M/9.94G [00:14<02:44, 56.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   7%|▋         | 650M/9.94G [00:14<03:33, 43.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   7%|▋         | 661M/9.94G [00:14<03:30, 44.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   7%|▋         | 682M/9.94G [00:15<02:56, 52.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   7%|▋         | 692M/9.94G [00:15<03:29, 44.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   7%|▋         | 713M/9.94G [00:15<02:50, 54.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   7%|▋         | 724M/9.94G [00:16<03:21, 45.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   7%|▋         | 744M/9.94G [00:16<03:20, 45.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   8%|▊         | 755M/9.94G [00:16<03:40, 41.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   8%|▊         | 776M/9.94G [00:17<03:06, 49.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   8%|▊         | 786M/9.94G [00:17<03:08, 48.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   8%|▊         | 807M/9.94G [00:17<03:07, 48.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   8%|▊         | 818M/9.94G [00:18<03:08, 48.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   8%|▊         | 828M/9.94G [00:18<02:57, 51.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   8%|▊         | 839M/9.94G [00:18<03:20, 45.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   9%|▊         | 849M/9.94G [00:19<04:45, 31.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   9%|▉         | 870M/9.94G [00:19<03:40, 41.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   9%|▉         | 881M/9.94G [00:20<04:27, 33.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   9%|▉         | 902M/9.94G [00:20<03:41, 40.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   9%|▉         | 912M/9.94G [00:20<03:56, 38.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   9%|▉         | 933M/9.94G [00:21<04:06, 36.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:   9%|▉         | 944M/9.94G [00:21<03:54, 38.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  10%|▉         | 954M/9.94G [00:21<03:49, 39.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  10%|▉         | 965M/9.94G [00:22<04:03, 36.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  10%|▉         | 975M/9.94G [00:22<03:59, 37.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  10%|▉         | 986M/9.94G [00:22<04:18, 34.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  10%|█         | 996M/9.94G [00:23<04:23, 33.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  10%|█         | 1.01G/9.94G [00:23<04:15, 35.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  10%|█         | 1.02G/9.94G [00:23<03:55, 37.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  10%|█         | 1.03G/9.94G [00:24<04:52, 30.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  11%|█         | 1.05G/9.94G [00:24<03:38, 40.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  11%|█         | 1.06G/9.94G [00:24<03:39, 40.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  11%|█         | 1.08G/9.94G [00:24<02:57, 49.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  11%|█         | 1.09G/9.94G [00:25<02:58, 49.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  11%|█         | 1.10G/9.94G [00:25<03:10, 46.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  11%|█         | 1.11G/9.94G [00:25<02:54, 50.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  11%|█▏        | 1.12G/9.94G [00:25<02:33, 57.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  11%|█▏        | 1.14G/9.94G [00:26<02:16, 64.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  12%|█▏        | 1.15G/9.94G [00:26<03:01, 48.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  12%|█▏        | 1.17G/9.94G [00:27<03:30, 41.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  12%|█▏        | 1.18G/9.94G [00:27<03:41, 39.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  12%|█▏        | 1.20G/9.94G [00:27<03:21, 43.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  12%|█▏        | 1.21G/9.94G [00:27<03:44, 39.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  12%|█▏        | 1.22G/9.94G [00:28<03:55, 37.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  12%|█▏        | 1.24G/9.94G [00:28<03:12, 45.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  13%|█▎        | 1.25G/9.94G [00:28<03:12, 45.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  13%|█▎        | 1.26G/9.94G [00:28<03:07, 46.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  13%|█▎        | 1.27G/9.94G [00:29<03:33, 40.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  13%|█▎        | 1.28G/9.94G [00:29<03:52, 37.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  13%|█▎        | 1.29G/9.94G [00:29<03:25, 42.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  13%|█▎        | 1.30G/9.94G [00:29<03:13, 44.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  13%|█▎        | 1.32G/9.94G [00:30<02:27, 58.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  13%|█▎        | 1.33G/9.94G [00:30<03:09, 45.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  14%|█▎        | 1.35G/9.94G [00:30<02:39, 53.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  14%|█▎        | 1.36G/9.94G [00:31<02:49, 50.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  14%|█▍        | 1.38G/9.94G [00:31<02:27, 57.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  14%|█▍        | 1.39G/9.94G [00:31<02:50, 50.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  14%|█▍        | 1.41G/9.94G [00:31<02:47, 51.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  14%|█▍        | 1.42G/9.94G [00:32<03:12, 44.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  14%|█▍        | 1.44G/9.94G [00:32<02:16, 62.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  15%|█▍        | 1.45G/9.94G [00:32<02:12, 63.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  15%|█▍        | 1.46G/9.94G [00:32<02:51, 49.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  15%|█▍        | 1.47G/9.94G [00:33<02:40, 52.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  15%|█▍        | 1.49G/9.94G [00:33<02:01, 69.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  15%|█▌        | 1.51G/9.94G [00:33<01:43, 81.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  15%|█▌        | 1.52G/9.94G [00:33<02:04, 67.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  16%|█▌        | 1.54G/9.94G [00:33<01:52, 74.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  16%|█▌        | 1.55G/9.94G [00:34<02:05, 67.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  16%|█▌        | 1.56G/9.94G [00:34<02:26, 57.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  16%|█▌        | 1.57G/9.94G [00:34<02:45, 50.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  16%|█▌        | 1.59G/9.94G [00:35<03:03, 45.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  16%|█▌        | 1.60G/9.94G [00:35<03:05, 45.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  16%|█▌        | 1.61G/9.94G [00:35<03:17, 42.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  16%|█▋        | 1.63G/9.94G [00:35<03:06, 44.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  16%|█▋        | 1.64G/9.94G [00:36<03:09, 43.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  17%|█▋        | 1.66G/9.94G [00:36<02:43, 50.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  17%|█▋        | 1.67G/9.94G [00:36<02:46, 49.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  17%|█▋        | 1.69G/9.94G [00:37<02:57, 46.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  17%|█▋        | 1.70G/9.94G [00:37<02:53, 47.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  17%|█▋        | 1.72G/9.94G [00:37<03:02, 45.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  17%|█▋        | 1.73G/9.94G [00:38<03:54, 35.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  18%|█▊        | 1.75G/9.94G [00:38<03:42, 36.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  18%|█▊        | 1.76G/9.94G [00:39<03:21, 40.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  18%|█▊        | 1.78G/9.94G [00:39<02:39, 51.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  18%|█▊        | 1.79G/9.94G [00:39<03:07, 43.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  18%|█▊        | 1.81G/9.94G [00:40<02:57, 45.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  18%|█▊        | 1.82G/9.94G [00:40<03:12, 42.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  18%|█▊        | 1.84G/9.94G [00:40<02:47, 48.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  19%|█▊        | 1.85G/9.94G [00:40<02:54, 46.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  19%|█▊        | 1.86G/9.94G [00:41<02:53, 46.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  19%|█▉        | 1.87G/9.94G [00:41<02:51, 47.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  19%|█▉        | 1.88G/9.94G [00:41<02:40, 50.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  19%|█▉        | 1.89G/9.94G [00:42<03:48, 35.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  19%|█▉        | 1.90G/9.94G [00:42<03:20, 40.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  19%|█▉        | 1.91G/9.94G [00:42<03:58, 33.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  19%|█▉        | 1.93G/9.94G [00:42<03:08, 42.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  20%|█▉        | 1.94G/9.94G [00:43<03:23, 39.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  20%|█▉        | 1.95G/9.94G [00:43<03:26, 38.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  20%|█▉        | 1.96G/9.94G [00:43<03:15, 40.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  20%|█▉        | 1.97G/9.94G [00:44<03:11, 41.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  20%|█▉        | 1.98G/9.94G [00:44<02:41, 49.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  20%|██        | 1.99G/9.94G [00:44<02:42, 48.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  20%|██        | 2.00G/9.94G [00:44<02:45, 48.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  20%|██        | 2.01G/9.94G [00:44<02:20, 56.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  20%|██        | 2.02G/9.94G [00:44<02:45, 47.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  20%|██        | 2.03G/9.94G [00:45<03:28, 38.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  21%|██        | 2.06G/9.94G [00:45<03:17, 39.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  21%|██        | 2.07G/9.94G [00:46<03:24, 38.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  21%|██        | 2.09G/9.94G [00:46<02:52, 45.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  21%|██        | 2.10G/9.94G [00:46<02:52, 45.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  21%|██▏       | 2.12G/9.94G [00:47<02:44, 47.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  21%|██▏       | 2.13G/9.94G [00:47<03:10, 41.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  22%|██▏       | 2.15G/9.94G [00:47<02:49, 46.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  22%|██▏       | 2.16G/9.94G [00:48<02:59, 43.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  22%|██▏       | 2.18G/9.94G [00:48<03:10, 40.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  22%|██▏       | 2.19G/9.94G [00:49<03:18, 39.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  22%|██▏       | 2.20G/9.94G [00:49<03:01, 42.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  22%|██▏       | 2.21G/9.94G [00:49<03:11, 40.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  22%|██▏       | 2.23G/9.94G [00:49<02:49, 45.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  23%|██▎       | 2.24G/9.94G [00:50<03:20, 38.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  23%|██▎       | 2.26G/9.94G [00:50<02:52, 44.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  23%|██▎       | 2.28G/9.94G [00:51<03:09, 40.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  23%|██▎       | 2.30G/9.94G [00:51<03:05, 41.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  23%|██▎       | 2.31G/9.94G [00:51<03:13, 39.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  23%|██▎       | 2.33G/9.94G [00:52<02:42, 46.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  24%|██▎       | 2.34G/9.94G [00:52<02:56, 43.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  24%|██▎       | 2.36G/9.94G [00:52<02:47, 45.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  24%|██▍       | 2.37G/9.94G [00:53<02:44, 46.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  24%|██▍       | 2.39G/9.94G [00:53<02:48, 44.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  24%|██▍       | 2.40G/9.94G [00:53<03:00, 41.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  24%|██▍       | 2.42G/9.94G [00:54<02:26, 51.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  24%|██▍       | 2.43G/9.94G [00:54<02:50, 44.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  25%|██▍       | 2.45G/9.94G [00:54<02:12, 56.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  25%|██▍       | 2.46G/9.94G [00:55<02:49, 44.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  25%|██▍       | 2.47G/9.94G [00:55<02:27, 50.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  25%|██▍       | 2.49G/9.94G [00:55<02:43, 45.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  25%|██▌       | 2.50G/9.94G [00:55<02:21, 52.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  25%|██▌       | 2.51G/9.94G [00:55<02:26, 50.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  25%|██▌       | 2.52G/9.94G [00:56<02:55, 42.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  26%|██▌       | 2.54G/9.94G [00:56<02:17, 53.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  26%|██▌       | 2.55G/9.94G [00:56<02:37, 46.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  26%|██▌       | 2.57G/9.94G [00:57<02:29, 49.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  26%|██▌       | 2.58G/9.94G [00:57<02:55, 41.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  26%|██▌       | 2.60G/9.94G [00:58<02:40, 45.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  26%|██▋       | 2.61G/9.94G [00:58<02:38, 46.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  26%|██▋       | 2.63G/9.94G [00:58<02:49, 43.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  27%|██▋       | 2.64G/9.94G [00:59<02:58, 40.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  27%|██▋       | 2.66G/9.94G [00:59<02:34, 47.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  27%|██▋       | 2.67G/9.94G [01:00<04:30, 26.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  27%|██▋       | 2.69G/9.94G [01:00<03:06, 38.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  27%|██▋       | 2.71G/9.94G [01:00<02:50, 42.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  27%|██▋       | 2.72G/9.94G [01:01<03:01, 39.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  27%|██▋       | 2.73G/9.94G [01:01<02:42, 44.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  28%|██▊       | 2.74G/9.94G [01:01<03:22, 35.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  28%|██▊       | 2.76G/9.94G [01:01<02:37, 45.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  28%|██▊       | 2.77G/9.94G [01:02<02:43, 43.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  28%|██▊       | 2.79G/9.94G [01:02<02:31, 47.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  28%|██▊       | 2.80G/9.94G [01:02<02:23, 49.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  28%|██▊       | 2.81G/9.94G [01:03<03:01, 39.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  28%|██▊       | 2.82G/9.94G [01:03<03:06, 38.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  29%|██▊       | 2.84G/9.94G [01:04<03:40, 32.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  29%|██▊       | 2.85G/9.94G [01:04<03:48, 31.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  29%|██▉       | 2.86G/9.94G [01:05<03:51, 30.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  29%|██▉       | 2.87G/9.94G [01:05<03:27, 34.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  29%|██▉       | 2.88G/9.94G [01:05<03:33, 33.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  29%|██▉       | 2.90G/9.94G [01:06<03:08, 37.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  29%|██▉       | 2.92G/9.94G [01:06<03:13, 36.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  29%|██▉       | 2.93G/9.94G [01:06<03:09, 37.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  30%|██▉       | 2.94G/9.94G [01:06<03:15, 35.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  30%|██▉       | 2.95G/9.94G [01:07<02:51, 40.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  30%|██▉       | 2.97G/9.94G [01:07<03:00, 38.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  30%|██▉       | 2.98G/9.94G [01:08<03:38, 31.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  30%|███       | 3.00G/9.94G [01:08<03:18, 35.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  30%|███       | 3.01G/9.94G [01:09<03:30, 32.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  30%|███       | 3.03G/9.94G [01:09<02:56, 39.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  31%|███       | 3.04G/9.94G [01:09<03:20, 34.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  31%|███       | 3.06G/9.94G [01:10<02:56, 38.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  31%|███       | 3.07G/9.94G [01:10<03:17, 34.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  31%|███       | 3.09G/9.94G [01:11<03:14, 35.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  31%|███       | 3.10G/9.94G [01:11<03:14, 35.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  31%|███▏      | 3.11G/9.94G [01:11<02:54, 39.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  31%|███▏      | 3.12G/9.94G [01:12<03:02, 37.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  32%|███▏      | 3.15G/9.94G [01:12<02:17, 49.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  32%|███▏      | 3.16G/9.94G [01:12<02:20, 48.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  32%|███▏      | 3.18G/9.94G [01:13<02:16, 49.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  32%|███▏      | 3.19G/9.94G [01:13<02:25, 46.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  32%|███▏      | 3.21G/9.94G [01:13<02:16, 49.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  32%|███▏      | 3.22G/9.94G [01:14<02:48, 39.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  33%|███▎      | 3.24G/9.94G [01:14<02:36, 42.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  33%|███▎      | 3.25G/9.94G [01:15<03:10, 35.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  33%|███▎      | 3.26G/9.94G [01:15<02:53, 38.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  33%|███▎      | 3.27G/9.94G [01:15<03:16, 33.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  33%|███▎      | 3.28G/9.94G [01:16<03:31, 31.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  33%|███▎      | 3.30G/9.94G [01:16<02:52, 38.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  33%|███▎      | 3.31G/9.94G [01:16<03:02, 36.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  34%|███▎      | 3.33G/9.94G [01:17<02:33, 43.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  34%|███▎      | 3.34G/9.94G [01:17<02:43, 40.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  34%|███▍      | 3.37G/9.94G [01:17<02:17, 47.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  34%|███▍      | 3.38G/9.94G [01:17<02:08, 51.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  34%|███▍      | 3.39G/9.94G [01:18<01:57, 55.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  34%|███▍      | 3.40G/9.94G [01:18<02:22, 45.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  34%|███▍      | 3.41G/9.94G [01:18<02:07, 51.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  34%|███▍      | 3.42G/9.94G [01:18<02:07, 51.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  34%|███▍      | 3.43G/9.94G [01:19<03:13, 33.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  35%|███▍      | 3.45G/9.94G [01:19<02:29, 43.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  35%|███▍      | 3.46G/9.94G [01:19<02:45, 39.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  35%|███▍      | 3.47G/9.94G [01:20<02:29, 43.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  35%|███▌      | 3.48G/9.94G [01:20<02:45, 39.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  35%|███▌      | 3.49G/9.94G [01:20<03:04, 34.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  35%|███▌      | 3.51G/9.94G [01:21<02:37, 40.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  35%|███▌      | 3.52G/9.94G [01:21<02:55, 36.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  36%|███▌      | 3.54G/9.94G [01:22<02:38, 40.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  36%|███▌      | 3.55G/9.94G [01:22<02:24, 44.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  36%|███▌      | 3.58G/9.94G [01:22<02:02, 52.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  36%|███▌      | 3.59G/9.94G [01:23<02:41, 39.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  36%|███▋      | 3.61G/9.94G [01:23<02:36, 40.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  36%|███▋      | 3.62G/9.94G [01:23<02:55, 36.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  37%|███▋      | 3.64G/9.94G [01:24<02:35, 40.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  37%|███▋      | 3.65G/9.94G [01:24<02:50, 37.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  37%|███▋      | 3.67G/9.94G [01:25<02:42, 38.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  37%|███▋      | 3.68G/9.94G [01:25<02:43, 38.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  37%|███▋      | 3.70G/9.94G [01:26<02:49, 36.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  37%|███▋      | 3.71G/9.94G [01:26<02:42, 38.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  37%|███▋      | 3.72G/9.94G [01:26<02:35, 40.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  38%|███▊      | 3.73G/9.94G [01:27<02:58, 34.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  38%|███▊      | 3.75G/9.94G [01:27<02:22, 43.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  38%|███▊      | 3.76G/9.94G [01:27<02:36, 39.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  38%|███▊      | 3.79G/9.94G [01:28<02:14, 45.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  38%|███▊      | 3.80G/9.94G [01:28<02:37, 39.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  38%|███▊      | 3.81G/9.94G [01:28<02:17, 44.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  38%|███▊      | 3.82G/9.94G [01:28<02:23, 42.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  38%|███▊      | 3.83G/9.94G [01:28<02:06, 48.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  39%|███▊      | 3.84G/9.94G [01:29<01:57, 52.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  39%|███▊      | 3.85G/9.94G [01:29<02:11, 46.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  39%|███▉      | 3.86G/9.94G [01:29<02:29, 40.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  39%|███▉      | 3.88G/9.94G [01:30<02:15, 44.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  39%|███▉      | 3.89G/9.94G [01:30<02:23, 42.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  39%|███▉      | 3.91G/9.94G [01:31<02:34, 39.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  39%|███▉      | 3.92G/9.94G [01:31<02:39, 37.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  40%|███▉      | 3.94G/9.94G [01:31<02:23, 41.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  40%|███▉      | 3.95G/9.94G [01:32<02:24, 41.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  40%|███▉      | 3.97G/9.94G [01:32<02:14, 44.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  40%|████      | 3.98G/9.94G [01:32<02:41, 36.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  40%|████      | 4.01G/9.94G [01:33<02:14, 44.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  40%|████      | 4.02G/9.94G [01:33<02:35, 38.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  41%|████      | 4.04G/9.94G [01:34<02:24, 41.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  41%|████      | 4.05G/9.94G [01:34<02:21, 41.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  41%|████      | 4.06G/9.94G [01:34<02:18, 42.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  41%|████      | 4.07G/9.94G [01:35<02:54, 33.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  41%|████      | 4.09G/9.94G [01:35<02:40, 36.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  41%|████      | 4.10G/9.94G [01:35<02:17, 42.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  41%|████▏     | 4.12G/9.94G [01:35<01:49, 53.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  42%|████▏     | 4.13G/9.94G [01:36<02:01, 47.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  42%|████▏     | 4.15G/9.94G [01:36<02:09, 44.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  42%|████▏     | 4.16G/9.94G [01:37<02:21, 41.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  42%|████▏     | 4.18G/9.94G [01:37<02:00, 47.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  42%|████▏     | 4.19G/9.94G [01:37<01:55, 49.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  42%|████▏     | 4.22G/9.94G [01:38<02:05, 45.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  42%|████▏     | 4.23G/9.94G [01:38<02:22, 40.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  43%|████▎     | 4.25G/9.94G [01:39<02:28, 38.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  43%|████▎     | 4.26G/9.94G [01:39<02:34, 36.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  43%|████▎     | 4.27G/9.94G [01:39<02:53, 32.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  43%|████▎     | 4.28G/9.94G [01:39<02:27, 38.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  43%|████▎     | 4.29G/9.94G [01:40<02:42, 34.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  43%|████▎     | 4.31G/9.94G [01:40<02:07, 44.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  43%|████▎     | 4.32G/9.94G [01:41<02:24, 38.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  44%|████▎     | 4.34G/9.94G [01:41<02:17, 40.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  44%|████▍     | 4.35G/9.94G [01:41<02:02, 45.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  44%|████▍     | 4.36G/9.94G [01:41<01:54, 48.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  44%|████▍     | 4.37G/9.94G [01:42<02:04, 44.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  44%|████▍     | 4.39G/9.94G [01:42<01:52, 49.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  44%|████▍     | 4.40G/9.94G [01:42<02:02, 45.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  45%|████▍     | 4.42G/9.94G [01:43<01:45, 52.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  45%|████▍     | 4.44G/9.94G [01:43<01:58, 46.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  45%|████▍     | 4.46G/9.94G [01:43<01:41, 54.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  45%|████▍     | 4.47G/9.94G [01:43<01:57, 46.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  45%|████▌     | 4.49G/9.94G [01:44<01:57, 46.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  45%|████▌     | 4.50G/9.94G [01:44<02:06, 43.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  45%|████▌     | 4.52G/9.94G [01:45<02:04, 43.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  46%|████▌     | 4.53G/9.94G [01:45<02:18, 39.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  46%|████▌     | 4.54G/9.94G [01:45<02:02, 44.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  46%|████▌     | 4.55G/9.94G [01:46<02:40, 33.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  46%|████▌     | 4.56G/9.94G [01:46<02:15, 39.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  46%|████▌     | 4.58G/9.94G [01:46<01:54, 46.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  46%|████▌     | 4.59G/9.94G [01:47<02:12, 40.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  46%|████▋     | 4.61G/9.94G [01:48<02:52, 31.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  47%|████▋     | 4.62G/9.94G [01:48<02:45, 32.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  47%|████▋     | 4.65G/9.94G [01:48<02:11, 40.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  47%|████▋     | 4.66G/9.94G [01:48<02:03, 42.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  47%|████▋     | 4.67G/9.94G [01:49<02:10, 40.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  47%|████▋     | 4.68G/9.94G [01:49<02:34, 34.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  47%|████▋     | 4.70G/9.94G [01:50<02:17, 38.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  47%|████▋     | 4.71G/9.94G [01:50<02:06, 41.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  48%|████▊     | 4.73G/9.94G [01:50<01:42, 50.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  48%|████▊     | 4.74G/9.94G [01:50<01:34, 55.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  48%|████▊     | 4.75G/9.94G [01:50<01:26, 60.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  48%|████▊     | 4.76G/9.94G [01:51<01:51, 46.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  48%|████▊     | 4.77G/9.94G [01:51<01:55, 44.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  48%|████▊     | 4.79G/9.94G [01:51<01:57, 43.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  48%|████▊     | 4.80G/9.94G [01:52<01:50, 46.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  49%|████▊     | 4.82G/9.94G [01:52<01:48, 47.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  49%|████▊     | 4.83G/9.94G [01:52<01:54, 44.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  49%|████▉     | 4.85G/9.94G [01:53<01:32, 54.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  49%|████▉     | 4.87G/9.94G [01:53<01:43, 48.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  49%|████▉     | 4.89G/9.94G [01:53<01:38, 51.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  49%|████▉     | 4.90G/9.94G [01:53<01:43, 48.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  49%|████▉     | 4.92G/9.94G [01:54<01:34, 53.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  50%|████▉     | 4.93G/9.94G [01:54<02:06, 39.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  50%|████▉     | 4.95G/9.94G [01:55<01:47, 46.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  50%|████▉     | 4.96G/9.94G [01:55<01:41, 48.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  50%|████▉     | 4.97G/9.94G [01:55<02:16, 36.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  50%|█████     | 4.98G/9.94G [01:56<02:38, 31.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  50%|█████     | 5.00G/9.94G [01:56<02:11, 37.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  50%|█████     | 5.01G/9.94G [01:57<02:19, 35.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  51%|█████     | 5.03G/9.94G [01:57<02:03, 39.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  51%|█████     | 5.04G/9.94G [01:57<02:08, 38.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  51%|█████     | 5.06G/9.94G [01:58<01:47, 45.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  51%|█████     | 5.08G/9.94G [01:58<01:50, 44.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  51%|█████▏    | 5.10G/9.94G [01:58<01:58, 40.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  51%|█████▏    | 5.11G/9.94G [01:59<02:07, 37.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  52%|█████▏    | 5.13G/9.94G [01:59<01:57, 41.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  52%|█████▏    | 5.14G/9.94G [01:59<01:57, 40.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  52%|█████▏    | 5.15G/9.94G [02:00<01:44, 45.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  52%|█████▏    | 5.16G/9.94G [02:00<01:46, 45.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  52%|█████▏    | 5.17G/9.94G [02:00<02:10, 36.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  52%|█████▏    | 5.19G/9.94G [02:01<01:59, 39.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  52%|█████▏    | 5.20G/9.94G [02:01<02:01, 39.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  52%|█████▏    | 5.21G/9.94G [02:01<01:45, 44.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  53%|█████▎    | 5.22G/9.94G [02:02<01:56, 40.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  53%|█████▎    | 5.23G/9.94G [02:02<02:31, 31.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  53%|█████▎    | 5.25G/9.94G [02:02<01:52, 41.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  53%|█████▎    | 5.27G/9.94G [02:03<01:31, 51.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  53%|█████▎    | 5.28G/9.94G [02:03<01:58, 39.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  53%|█████▎    | 5.30G/9.94G [02:03<01:42, 45.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  53%|█████▎    | 5.31G/9.94G [02:04<02:05, 36.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  54%|█████▎    | 5.33G/9.94G [02:04<01:25, 53.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  54%|█████▎    | 5.34G/9.94G [02:04<01:40, 45.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  54%|█████▍    | 5.35G/9.94G [02:04<01:40, 45.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  54%|█████▍    | 5.37G/9.94G [02:05<01:30, 50.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  54%|█████▍    | 5.38G/9.94G [02:05<01:41, 44.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  54%|█████▍    | 5.40G/9.94G [02:06<02:03, 36.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  54%|█████▍    | 5.41G/9.94G [02:06<02:22, 31.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  55%|█████▍    | 5.43G/9.94G [02:07<01:47, 41.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  55%|█████▍    | 5.44G/9.94G [02:07<01:47, 41.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  55%|█████▍    | 5.46G/9.94G [02:07<01:32, 48.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  55%|█████▌    | 5.47G/9.94G [02:07<01:30, 49.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  55%|█████▌    | 5.48G/9.94G [02:07<01:26, 51.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  55%|█████▌    | 5.49G/9.94G [02:08<01:27, 50.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  55%|█████▌    | 5.51G/9.94G [02:08<01:38, 45.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  56%|█████▌    | 5.53G/9.94G [02:08<01:27, 50.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  56%|█████▌    | 5.54G/9.94G [02:09<01:41, 43.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  56%|█████▌    | 5.56G/9.94G [02:09<01:45, 41.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  56%|█████▌    | 5.57G/9.94G [02:09<01:33, 47.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  56%|█████▌    | 5.58G/9.94G [02:10<01:26, 50.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  56%|█████▌    | 5.59G/9.94G [02:10<01:31, 47.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  56%|█████▋    | 5.61G/9.94G [02:10<01:53, 38.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  57%|█████▋    | 5.62G/9.94G [02:11<01:43, 41.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  57%|█████▋    | 5.64G/9.94G [02:11<01:31, 46.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  57%|█████▋    | 5.65G/9.94G [02:11<01:38, 43.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  57%|█████▋    | 5.67G/9.94G [02:12<01:33, 45.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  57%|█████▋    | 5.68G/9.94G [02:12<02:09, 33.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  57%|█████▋    | 5.69G/9.94G [02:13<02:03, 34.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  57%|█████▋    | 5.70G/9.94G [02:13<02:12, 32.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  57%|█████▋    | 5.71G/9.94G [02:13<02:14, 31.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  58%|█████▊    | 5.74G/9.94G [02:14<01:55, 36.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  58%|█████▊    | 5.75G/9.94G [02:14<02:15, 30.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  58%|█████▊    | 5.77G/9.94G [02:15<01:48, 38.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  58%|█████▊    | 5.78G/9.94G [02:15<01:49, 38.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  58%|█████▊    | 5.80G/9.94G [02:15<01:32, 44.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  58%|█████▊    | 5.81G/9.94G [02:16<01:40, 41.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  59%|█████▊    | 5.83G/9.94G [02:16<01:36, 42.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  59%|█████▊    | 5.84G/9.94G [02:16<01:43, 39.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  59%|█████▉    | 5.86G/9.94G [02:17<01:43, 39.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  59%|█████▉    | 5.87G/9.94G [02:17<01:39, 40.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  59%|█████▉    | 5.89G/9.94G [02:17<01:22, 49.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  59%|█████▉    | 5.91G/9.94G [02:18<01:05, 61.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  60%|█████▉    | 5.92G/9.94G [02:18<01:17, 51.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  60%|█████▉    | 5.95G/9.94G [02:18<01:16, 52.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  60%|█████▉    | 5.96G/9.94G [02:19<01:18, 50.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  60%|██████    | 5.98G/9.94G [02:19<01:19, 49.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  60%|██████    | 5.99G/9.94G [02:19<01:30, 43.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  60%|██████    | 6.00G/9.94G [02:20<01:21, 48.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  60%|██████    | 6.01G/9.94G [02:20<01:35, 41.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  61%|██████    | 6.02G/9.94G [02:20<01:30, 43.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  61%|██████    | 6.04G/9.94G [02:20<01:13, 53.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  61%|██████    | 6.05G/9.94G [02:21<01:19, 48.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  61%|██████    | 6.07G/9.94G [02:21<01:01, 63.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  61%|██████    | 6.08G/9.94G [02:21<01:11, 54.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  61%|██████▏   | 6.10G/9.94G [02:22<01:18, 49.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  61%|██████▏   | 6.11G/9.94G [02:22<01:25, 45.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  62%|██████▏   | 6.12G/9.94G [02:22<01:24, 45.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  62%|██████▏   | 6.13G/9.94G [02:22<01:23, 45.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  62%|██████▏   | 6.14G/9.94G [02:23<01:36, 39.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  62%|██████▏   | 6.17G/9.94G [02:23<01:20, 47.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  62%|██████▏   | 6.18G/9.94G [02:24<01:39, 37.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  62%|██████▏   | 6.19G/9.94G [02:24<01:25, 44.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  62%|██████▏   | 6.20G/9.94G [02:24<01:36, 38.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  62%|██████▏   | 6.21G/9.94G [02:24<01:21, 45.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  63%|██████▎   | 6.22G/9.94G [02:24<01:14, 49.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  63%|██████▎   | 6.23G/9.94G [02:25<01:27, 42.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  63%|██████▎   | 6.25G/9.94G [02:25<01:18, 46.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  63%|██████▎   | 6.26G/9.94G [02:25<01:21, 45.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  63%|██████▎   | 6.28G/9.94G [02:26<01:14, 49.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  63%|██████▎   | 6.29G/9.94G [02:26<01:08, 53.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  63%|██████▎   | 6.31G/9.94G [02:26<01:13, 49.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  64%|██████▎   | 6.32G/9.94G [02:27<01:18, 46.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  64%|██████▍   | 6.34G/9.94G [02:27<01:14, 48.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  64%|██████▍   | 6.35G/9.94G [02:27<01:20, 44.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  64%|██████▍   | 6.38G/9.94G [02:28<01:12, 49.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  64%|██████▍   | 6.39G/9.94G [02:28<01:30, 39.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  64%|██████▍   | 6.41G/9.94G [02:29<01:36, 36.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  65%|██████▍   | 6.42G/9.94G [02:29<01:34, 37.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  65%|██████▍   | 6.44G/9.94G [02:30<01:45, 33.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  65%|██████▍   | 6.45G/9.94G [02:30<01:44, 33.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  65%|██████▌   | 6.47G/9.94G [02:30<01:28, 39.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  65%|██████▌   | 6.48G/9.94G [02:31<01:35, 36.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  65%|██████▌   | 6.50G/9.94G [02:31<01:16, 45.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  65%|██████▌   | 6.51G/9.94G [02:31<01:22, 41.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  66%|██████▌   | 6.52G/9.94G [02:32<01:26, 39.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  66%|██████▌   | 6.53G/9.94G [02:32<01:19, 43.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  66%|██████▌   | 6.54G/9.94G [02:32<01:24, 40.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  66%|██████▌   | 6.55G/9.94G [02:32<01:22, 41.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  66%|██████▌   | 6.56G/9.94G [02:33<01:43, 32.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  66%|██████▌   | 6.59G/9.94G [02:33<01:12, 46.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  66%|██████▋   | 6.60G/9.94G [02:34<01:30, 37.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  67%|██████▋   | 6.62G/9.94G [02:34<01:19, 41.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  67%|██████▋   | 6.63G/9.94G [02:34<01:16, 43.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  67%|██████▋   | 6.65G/9.94G [02:35<01:17, 42.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  67%|██████▋   | 6.67G/9.94G [02:35<00:56, 57.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  67%|██████▋   | 6.68G/9.94G [02:35<01:13, 44.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  67%|██████▋   | 6.69G/9.94G [02:36<01:16, 42.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  67%|██████▋   | 6.71G/9.94G [02:36<01:09, 46.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  68%|██████▊   | 6.72G/9.94G [02:36<01:16, 42.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  68%|██████▊   | 6.74G/9.94G [02:37<01:11, 44.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  68%|██████▊   | 6.75G/9.94G [02:37<01:23, 38.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  68%|██████▊   | 6.77G/9.94G [02:38<01:13, 42.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  68%|██████▊   | 6.78G/9.94G [02:38<01:11, 44.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  68%|██████▊   | 6.81G/9.94G [02:38<01:03, 49.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  69%|██████▊   | 6.82G/9.94G [02:38<01:08, 45.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  69%|██████▊   | 6.83G/9.94G [02:39<01:09, 45.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  69%|██████▉   | 6.84G/9.94G [02:39<01:30, 34.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  69%|██████▉   | 6.86G/9.94G [02:39<01:04, 47.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  69%|██████▉   | 6.87G/9.94G [02:40<01:05, 46.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  69%|██████▉   | 6.89G/9.94G [02:40<00:59, 51.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  69%|██████▉   | 6.90G/9.94G [02:40<00:55, 54.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  70%|██████▉   | 6.92G/9.94G [02:40<00:54, 55.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  70%|██████▉   | 6.93G/9.94G [02:41<01:05, 45.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  70%|██████▉   | 6.95G/9.94G [02:41<00:59, 50.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  70%|███████   | 6.96G/9.94G [02:41<01:06, 44.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  70%|███████   | 6.97G/9.94G [02:42<01:04, 45.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  70%|███████   | 6.98G/9.94G [02:42<01:09, 42.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  70%|███████   | 6.99G/9.94G [02:42<01:20, 36.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  71%|███████   | 7.01G/9.94G [02:43<01:13, 39.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  71%|███████   | 7.03G/9.94G [02:43<01:20, 36.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  71%|███████   | 7.05G/9.94G [02:44<01:15, 38.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  71%|███████   | 7.06G/9.94G [02:44<01:35, 30.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  71%|███████   | 7.08G/9.94G [02:45<01:24, 33.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  71%|███████▏  | 7.09G/9.94G [02:45<01:18, 36.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  72%|███████▏  | 7.11G/9.94G [02:45<00:57, 49.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  72%|███████▏  | 7.12G/9.94G [02:45<00:57, 49.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  72%|███████▏  | 7.13G/9.94G [02:46<00:56, 49.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  72%|███████▏  | 7.14G/9.94G [02:46<00:58, 47.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  72%|███████▏  | 7.15G/9.94G [02:46<00:51, 54.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  72%|███████▏  | 7.16G/9.94G [02:46<00:51, 53.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  72%|███████▏  | 7.17G/9.94G [02:47<01:06, 41.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  72%|███████▏  | 7.19G/9.94G [02:47<00:51, 53.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  72%|███████▏  | 7.20G/9.94G [02:47<00:56, 48.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  73%|███████▎  | 7.22G/9.94G [02:47<00:49, 54.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  73%|███████▎  | 7.24G/9.94G [02:48<00:59, 45.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  73%|███████▎  | 7.25G/9.94G [02:48<01:01, 44.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  73%|███████▎  | 7.26G/9.94G [02:48<00:58, 46.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  73%|███████▎  | 7.27G/9.94G [02:49<01:05, 40.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  73%|███████▎  | 7.29G/9.94G [02:49<00:54, 48.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  73%|███████▎  | 7.30G/9.94G [02:49<00:53, 49.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  74%|███████▎  | 7.31G/9.94G [02:49<00:53, 49.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  74%|███████▎  | 7.32G/9.94G [02:50<01:00, 43.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  74%|███████▎  | 7.33G/9.94G [02:50<00:54, 47.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  74%|███████▍  | 7.35G/9.94G [02:50<00:45, 57.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  74%|███████▍  | 7.36G/9.94G [02:50<00:44, 58.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  74%|███████▍  | 7.37G/9.94G [02:51<00:52, 48.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  74%|███████▍  | 7.38G/9.94G [02:51<00:50, 51.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  74%|███████▍  | 7.39G/9.94G [02:51<00:50, 50.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  75%|███████▍  | 7.41G/9.94G [02:52<01:02, 40.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  75%|███████▍  | 7.42G/9.94G [02:52<00:56, 44.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  75%|███████▍  | 7.43G/9.94G [02:52<01:06, 37.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  75%|███████▍  | 7.44G/9.94G [02:52<00:57, 43.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  75%|███████▌  | 7.47G/9.94G [02:53<00:47, 51.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  75%|███████▌  | 7.48G/9.94G [02:53<00:47, 52.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  75%|███████▌  | 7.50G/9.94G [02:53<00:45, 54.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  76%|███████▌  | 7.51G/9.94G [02:54<00:59, 40.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  76%|███████▌  | 7.53G/9.94G [02:54<00:58, 41.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  76%|███████▌  | 7.54G/9.94G [02:55<01:05, 37.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  76%|███████▌  | 7.56G/9.94G [02:55<01:03, 37.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  76%|███████▌  | 7.57G/9.94G [02:55<01:00, 39.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  76%|███████▋  | 7.59G/9.94G [02:56<00:59, 39.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  76%|███████▋  | 7.60G/9.94G [02:56<01:02, 37.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  77%|███████▋  | 7.62G/9.94G [02:57<01:05, 35.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  77%|███████▋  | 7.63G/9.94G [02:57<01:19, 28.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  77%|███████▋  | 7.65G/9.94G [02:58<01:06, 34.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  77%|███████▋  | 7.67G/9.94G [02:58<01:06, 34.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  77%|███████▋  | 7.69G/9.94G [02:59<01:05, 34.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  77%|███████▋  | 7.70G/9.94G [02:59<01:03, 35.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  78%|███████▊  | 7.71G/9.94G [02:59<00:56, 39.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  78%|███████▊  | 7.72G/9.94G [03:00<01:07, 32.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  78%|███████▊  | 7.73G/9.94G [03:00<01:07, 33.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  78%|███████▊  | 7.75G/9.94G [03:01<01:00, 36.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  78%|███████▊  | 7.76G/9.94G [03:01<00:51, 42.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  78%|███████▊  | 7.77G/9.94G [03:01<00:54, 40.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  78%|███████▊  | 7.78G/9.94G [03:01<00:55, 38.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  78%|███████▊  | 7.80G/9.94G [03:02<00:47, 45.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  79%|███████▊  | 7.81G/9.94G [03:02<00:48, 43.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  79%|███████▉  | 7.83G/9.94G [03:02<00:43, 48.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  79%|███████▉  | 7.84G/9.94G [03:02<00:42, 48.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  79%|███████▉  | 7.85G/9.94G [03:03<00:43, 48.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  79%|███████▉  | 7.86G/9.94G [03:03<00:48, 43.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  79%|███████▉  | 7.87G/9.94G [03:03<00:51, 39.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  79%|███████▉  | 7.90G/9.94G [03:04<00:40, 50.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  80%|███████▉  | 7.91G/9.94G [03:04<00:44, 45.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  80%|███████▉  | 7.93G/9.94G [03:04<00:42, 46.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  80%|███████▉  | 7.94G/9.94G [03:05<00:44, 44.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  80%|████████  | 7.96G/9.94G [03:05<00:47, 42.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  80%|████████  | 7.97G/9.94G [03:05<00:52, 37.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  80%|████████  | 7.98G/9.94G [03:06<00:44, 43.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  80%|████████  | 7.99G/9.94G [03:06<00:50, 38.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  80%|████████  | 8.00G/9.94G [03:06<00:55, 35.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  81%|████████  | 8.01G/9.94G [03:06<00:45, 42.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  81%|████████  | 8.02G/9.94G [03:07<00:55, 34.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  81%|████████  | 8.03G/9.94G [03:07<01:03, 30.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  81%|████████  | 8.04G/9.94G [03:08<00:54, 35.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  81%|████████  | 8.05G/9.94G [03:08<01:02, 30.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  81%|████████  | 8.06G/9.94G [03:08<00:51, 36.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  81%|████████  | 8.07G/9.94G [03:08<00:52, 35.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  81%|████████▏ | 8.08G/9.94G [03:09<00:46, 39.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  82%|████████▏ | 8.11G/9.94G [03:09<00:41, 44.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  82%|████████▏ | 8.12G/9.94G [03:09<00:50, 36.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  82%|████████▏ | 8.14G/9.94G [03:10<00:46, 38.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  82%|████████▏ | 8.15G/9.94G [03:10<00:43, 41.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  82%|████████▏ | 8.17G/9.94G [03:11<00:39, 44.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  82%|████████▏ | 8.18G/9.94G [03:11<00:38, 45.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  82%|████████▏ | 8.20G/9.94G [03:11<00:32, 53.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  83%|████████▎ | 8.21G/9.94G [03:12<00:43, 40.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  83%|████████▎ | 8.23G/9.94G [03:12<00:32, 52.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  83%|████████▎ | 8.24G/9.94G [03:12<00:41, 41.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  83%|████████▎ | 8.26G/9.94G [03:13<00:35, 47.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  83%|████████▎ | 8.27G/9.94G [03:13<00:39, 41.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  83%|████████▎ | 8.29G/9.94G [03:13<00:35, 45.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  84%|████████▎ | 8.30G/9.94G [03:14<00:40, 40.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  84%|████████▎ | 8.33G/9.94G [03:14<00:39, 41.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  84%|████████▍ | 8.34G/9.94G [03:15<00:44, 35.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  84%|████████▍ | 8.36G/9.94G [03:15<00:38, 40.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  84%|████████▍ | 8.37G/9.94G [03:15<00:36, 43.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  84%|████████▍ | 8.38G/9.94G [03:16<00:41, 38.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  84%|████████▍ | 8.39G/9.94G [03:16<00:44, 35.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  85%|████████▍ | 8.41G/9.94G [03:17<00:43, 35.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  85%|████████▍ | 8.42G/9.94G [03:17<00:40, 37.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  85%|████████▍ | 8.43G/9.94G [03:17<00:43, 34.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  85%|████████▍ | 8.44G/9.94G [03:17<00:41, 36.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  85%|████████▍ | 8.45G/9.94G [03:18<00:45, 32.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  85%|████████▌ | 8.47G/9.94G [03:18<00:35, 41.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  85%|████████▌ | 8.48G/9.94G [03:18<00:31, 46.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  86%|████████▌ | 8.50G/9.94G [03:19<00:25, 56.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  86%|████████▌ | 8.51G/9.94G [03:19<00:30, 47.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  86%|████████▌ | 8.54G/9.94G [03:19<00:32, 43.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  86%|████████▌ | 8.55G/9.94G [03:20<00:31, 43.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  86%|████████▌ | 8.56G/9.94G [03:20<00:27, 50.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  86%|████████▌ | 8.57G/9.94G [03:20<00:31, 44.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  86%|████████▋ | 8.58G/9.94G [03:20<00:35, 38.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  86%|████████▋ | 8.60G/9.94G [03:21<00:30, 43.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  87%|████████▋ | 8.61G/9.94G [03:21<00:30, 44.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  87%|████████▋ | 8.63G/9.94G [03:21<00:27, 47.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  87%|████████▋ | 8.64G/9.94G [03:22<00:30, 43.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  87%|████████▋ | 8.66G/9.94G [03:22<00:26, 47.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  87%|████████▋ | 8.67G/9.94G [03:22<00:23, 54.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  87%|████████▋ | 8.68G/9.94G [03:22<00:23, 53.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  87%|████████▋ | 8.69G/9.94G [03:23<00:33, 37.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  88%|████████▊ | 8.71G/9.94G [03:23<00:31, 39.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  88%|████████▊ | 8.72G/9.94G [03:24<00:30, 39.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  88%|████████▊ | 8.75G/9.94G [03:24<00:22, 52.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  88%|████████▊ | 8.76G/9.94G [03:24<00:23, 50.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  88%|████████▊ | 8.78G/9.94G [03:25<00:22, 52.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  88%|████████▊ | 8.79G/9.94G [03:25<00:27, 42.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  89%|████████▊ | 8.81G/9.94G [03:25<00:21, 52.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  89%|████████▊ | 8.82G/9.94G [03:25<00:20, 53.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  89%|████████▉ | 8.84G/9.94G [03:26<00:18, 60.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  89%|████████▉ | 8.85G/9.94G [03:26<00:22, 48.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  89%|████████▉ | 8.87G/9.94G [03:26<00:22, 48.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  89%|████████▉ | 8.88G/9.94G [03:27<00:24, 42.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  90%|████████▉ | 8.90G/9.94G [03:27<00:23, 44.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  90%|████████▉ | 8.91G/9.94G [03:28<00:25, 40.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  90%|████████▉ | 8.93G/9.94G [03:28<00:23, 42.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  90%|████████▉ | 8.94G/9.94G [03:28<00:26, 37.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  90%|█████████ | 8.97G/9.94G [03:29<00:26, 36.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  90%|█████████ | 8.98G/9.94G [03:29<00:25, 37.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  90%|█████████ | 8.99G/9.94G [03:29<00:23, 41.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  90%|█████████ | 9.00G/9.94G [03:30<00:23, 41.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  91%|█████████ | 9.02G/9.94G [03:30<00:21, 42.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  91%|█████████ | 9.03G/9.94G [03:31<00:23, 39.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  91%|█████████ | 9.04G/9.94G [03:31<00:19, 45.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  91%|█████████ | 9.05G/9.94G [03:31<00:21, 41.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  91%|█████████ | 9.06G/9.94G [03:31<00:21, 41.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  91%|█████████ | 9.07G/9.94G [03:31<00:20, 42.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  91%|█████████▏| 9.08G/9.94G [03:32<00:20, 41.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  91%|█████████▏| 9.09G/9.94G [03:32<00:22, 38.0MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  92%|█████████▏| 9.11G/9.94G [03:32<00:18, 44.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  92%|█████████▏| 9.12G/9.94G [03:33<00:21, 38.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  92%|█████████▏| 9.14G/9.94G [03:33<00:18, 43.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  92%|█████████▏| 9.15G/9.94G [03:34<00:21, 37.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  92%|█████████▏| 9.18G/9.94G [03:34<00:18, 42.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  92%|█████████▏| 9.19G/9.94G [03:34<00:19, 39.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  93%|█████████▎| 9.21G/9.94G [03:35<00:16, 44.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  93%|█████████▎| 9.22G/9.94G [03:35<00:18, 40.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  93%|█████████▎| 9.23G/9.94G [03:35<00:15, 45.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  93%|█████████▎| 9.24G/9.94G [03:36<00:18, 38.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  93%|█████████▎| 9.25G/9.94G [03:36<00:21, 31.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  93%|█████████▎| 9.27G/9.94G [03:36<00:16, 41.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  93%|█████████▎| 9.29G/9.94G [03:37<00:12, 51.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  94%|█████████▎| 9.30G/9.94G [03:37<00:13, 47.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  94%|█████████▍| 9.32G/9.94G [03:37<00:11, 52.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  94%|█████████▍| 9.33G/9.94G [03:38<00:15, 39.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  94%|█████████▍| 9.34G/9.94G [03:38<00:17, 34.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  94%|█████████▍| 9.35G/9.94G [03:38<00:15, 37.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  94%|█████████▍| 9.36G/9.94G [03:39<00:15, 38.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  94%|█████████▍| 9.38G/9.94G [03:39<00:11, 47.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  94%|█████████▍| 9.40G/9.94G [03:39<00:12, 45.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  95%|█████████▍| 9.41G/9.94G [03:39<00:11, 47.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  95%|█████████▍| 9.42G/9.94G [03:40<00:10, 51.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  95%|█████████▍| 9.43G/9.94G [03:40<00:11, 43.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  95%|█████████▌| 9.45G/9.94G [03:40<00:09, 50.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  95%|█████████▌| 9.46G/9.94G [03:41<00:10, 45.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  95%|█████████▌| 9.48G/9.94G [03:41<00:11, 38.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  95%|█████████▌| 9.49G/9.94G [03:41<00:10, 41.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  96%|█████████▌| 9.51G/9.94G [03:42<00:08, 49.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  96%|█████████▌| 9.52G/9.94G [03:42<00:08, 48.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  96%|█████████▌| 9.53G/9.94G [03:42<00:07, 54.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  96%|█████████▌| 9.54G/9.94G [03:42<00:08, 47.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  96%|█████████▌| 9.55G/9.94G [03:43<00:08, 44.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  96%|█████████▋| 9.57G/9.94G [03:43<00:08, 45.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  96%|█████████▋| 9.58G/9.94G [03:43<00:07, 46.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  96%|█████████▋| 9.59G/9.94G [03:44<00:07, 46.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  97%|█████████▋| 9.60G/9.94G [03:44<00:09, 35.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  97%|█████████▋| 9.63G/9.94G [03:44<00:07, 40.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  97%|█████████▋| 9.64G/9.94G [03:45<00:07, 40.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  97%|█████████▋| 9.65G/9.94G [03:45<00:06, 45.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  97%|█████████▋| 9.66G/9.94G [03:45<00:06, 40.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  97%|█████████▋| 9.67G/9.94G [03:46<00:07, 36.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  97%|█████████▋| 9.68G/9.94G [03:46<00:07, 35.2MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  97%|█████████▋| 9.69G/9.94G [03:46<00:07, 32.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  98%|█████████▊| 9.70G/9.94G [03:47<00:08, 29.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  98%|█████████▊| 9.72G/9.94G [03:47<00:06, 35.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  98%|█████████▊| 9.73G/9.94G [03:47<00:06, 34.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  98%|█████████▊| 9.75G/9.94G [03:48<00:04, 40.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  98%|█████████▊| 9.76G/9.94G [03:48<00:04, 37.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  98%|█████████▊| 9.78G/9.94G [03:48<00:03, 48.4MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  98%|█████████▊| 9.79G/9.94G [03:49<00:03, 45.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  99%|█████████▊| 9.80G/9.94G [03:49<00:02, 50.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  99%|█████████▊| 9.81G/9.94G [03:49<00:03, 38.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  99%|█████████▉| 9.83G/9.94G [03:50<00:03, 34.6MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  99%|█████████▉| 9.85G/9.94G [03:50<00:02, 41.5MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  99%|█████████▉| 9.86G/9.94G [03:51<00:02, 32.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  99%|█████████▉| 9.88G/9.94G [03:51<00:01, 37.3MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin:  99%|█████████▉| 9.89G/9.94G [03:51<00:01, 36.1MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin: 100%|█████████▉| 9.91G/9.94G [03:52<00:00, 41.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin: 100%|█████████▉| 9.92G/9.94G [03:52<00:00, 46.9MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin: 100%|█████████▉| 9.93G/9.94G [03:52<00:00, 47.8MB/s]\u001b[A\nDownloading (…)l-00001-of-00002.bin: 100%|██████████| 9.94G/9.94G [03:52<00:00, 42.7MB/s]\u001b[A\nDownloading shards:  50%|█████     | 1/2 [03:53<03:53, 233.51s/it]\nDownloading (…)l-00002-of-00002.bin:   0%|          | 0.00/4.54G [00:00<?, ?B/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:   0%|          | 21.0M/4.54G [00:00<00:26, 172MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:   1%|          | 41.9M/4.54G [00:00<00:23, 189MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:   1%|▏         | 62.9M/4.54G [00:00<00:22, 196MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:   2%|▏         | 83.9M/4.54G [00:00<00:22, 198MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:   2%|▏         | 105M/4.54G [00:00<00:22, 196MB/s] \u001b[A\nDownloading (…)l-00002-of-00002.bin:   3%|▎         | 126M/4.54G [00:00<00:22, 197MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:   3%|▎         | 147M/4.54G [00:00<00:22, 197MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:   4%|▎         | 168M/4.54G [00:00<00:22, 197MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:   4%|▍         | 189M/4.54G [00:00<00:21, 198MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:   5%|▍         | 210M/4.54G [00:01<00:21, 199MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:   5%|▌         | 231M/4.54G [00:01<00:21, 200MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:   6%|▌         | 252M/4.54G [00:01<00:21, 201MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:   6%|▌         | 273M/4.54G [00:01<00:21, 201MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:   6%|▋         | 294M/4.54G [00:01<00:21, 201MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:   7%|▋         | 315M/4.54G [00:01<00:21, 201MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:   7%|▋         | 336M/4.54G [00:01<00:21, 200MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:   8%|▊         | 357M/4.54G [00:01<00:20, 200MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:   8%|▊         | 377M/4.54G [00:01<00:20, 201MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:   9%|▉         | 398M/4.54G [00:02<00:20, 202MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:   9%|▉         | 419M/4.54G [00:02<00:20, 203MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  10%|▉         | 440M/4.54G [00:02<00:20, 204MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  10%|█         | 461M/4.54G [00:02<00:19, 204MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  11%|█         | 482M/4.54G [00:02<00:19, 203MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  11%|█         | 503M/4.54G [00:02<00:19, 202MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  12%|█▏        | 524M/4.54G [00:02<00:19, 202MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  12%|█▏        | 545M/4.54G [00:02<00:19, 201MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  12%|█▏        | 566M/4.54G [00:02<00:19, 201MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  13%|█▎        | 587M/4.54G [00:02<00:20, 192MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  13%|█▎        | 608M/4.54G [00:03<00:20, 195MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  14%|█▍        | 629M/4.54G [00:03<00:19, 197MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  14%|█▍        | 650M/4.54G [00:03<00:19, 198MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  15%|█▍        | 671M/4.54G [00:03<00:19, 199MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  15%|█▌        | 692M/4.54G [00:03<00:19, 200MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  16%|█▌        | 713M/4.54G [00:03<00:19, 199MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  16%|█▌        | 734M/4.54G [00:03<00:19, 200MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  17%|█▋        | 755M/4.54G [00:03<00:18, 200MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  17%|█▋        | 776M/4.54G [00:03<00:18, 200MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  18%|█▊        | 797M/4.54G [00:03<00:18, 201MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  18%|█▊        | 818M/4.54G [00:04<00:18, 202MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  18%|█▊        | 839M/4.54G [00:04<00:18, 202MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  19%|█▉        | 860M/4.54G [00:04<00:18, 202MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  19%|█▉        | 881M/4.54G [00:04<00:18, 202MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  20%|█▉        | 902M/4.54G [00:04<00:18, 200MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  20%|██        | 923M/4.54G [00:04<00:18, 200MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  21%|██        | 944M/4.54G [00:04<00:18, 199MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  21%|██        | 965M/4.54G [00:04<00:17, 200MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  22%|██▏       | 986M/4.54G [00:04<00:17, 201MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  22%|██▏       | 1.01G/4.54G [00:05<00:17, 201MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  23%|██▎       | 1.03G/4.54G [00:05<00:17, 200MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  23%|██▎       | 1.05G/4.54G [00:05<00:17, 199MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  24%|██▎       | 1.07G/4.54G [00:05<00:17, 199MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  24%|██▍       | 1.09G/4.54G [00:05<00:17, 198MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  24%|██▍       | 1.11G/4.54G [00:05<00:17, 199MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  25%|██▍       | 1.13G/4.54G [00:05<00:17, 199MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  25%|██▌       | 1.15G/4.54G [00:05<00:16, 199MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  26%|██▌       | 1.17G/4.54G [00:05<00:16, 199MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  26%|██▋       | 1.20G/4.54G [00:05<00:16, 199MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  27%|██▋       | 1.22G/4.54G [00:06<00:16, 199MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  27%|██▋       | 1.24G/4.54G [00:06<00:16, 200MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  28%|██▊       | 1.26G/4.54G [00:06<00:16, 201MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  28%|██▊       | 1.28G/4.54G [00:06<00:16, 201MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  29%|██▊       | 1.30G/4.54G [00:06<00:16, 201MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  29%|██▉       | 1.32G/4.54G [00:06<00:16, 200MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  30%|██▉       | 1.34G/4.54G [00:06<00:15, 200MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  30%|███       | 1.36G/4.54G [00:06<00:15, 200MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  30%|███       | 1.38G/4.54G [00:06<00:15, 200MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  31%|███       | 1.41G/4.54G [00:07<00:15, 199MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  31%|███▏      | 1.43G/4.54G [00:07<00:15, 200MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  32%|███▏      | 1.45G/4.54G [00:07<00:15, 199MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  32%|███▏      | 1.47G/4.54G [00:07<00:15, 200MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  33%|███▎      | 1.49G/4.54G [00:07<00:15, 199MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  33%|███▎      | 1.51G/4.54G [00:07<00:15, 198MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  34%|███▎      | 1.53G/4.54G [00:07<00:15, 198MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  34%|███▍      | 1.55G/4.54G [00:07<00:15, 198MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  35%|███▍      | 1.57G/4.54G [00:07<00:14, 199MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  35%|███▌      | 1.59G/4.54G [00:07<00:14, 199MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  36%|███▌      | 1.61G/4.54G [00:08<00:14, 200MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  36%|███▌      | 1.64G/4.54G [00:08<00:14, 200MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  36%|███▋      | 1.66G/4.54G [00:08<00:14, 200MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  37%|███▋      | 1.68G/4.54G [00:08<00:14, 200MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  37%|███▋      | 1.70G/4.54G [00:08<00:14, 201MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  38%|███▊      | 1.72G/4.54G [00:08<00:14, 201MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  38%|███▊      | 1.74G/4.54G [00:08<00:13, 201MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  39%|███▉      | 1.76G/4.54G [00:08<00:13, 202MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  39%|███▉      | 1.78G/4.54G [00:08<00:13, 201MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  40%|███▉      | 1.80G/4.54G [00:09<00:13, 201MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  40%|████      | 1.82G/4.54G [00:09<00:13, 201MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  41%|████      | 1.85G/4.54G [00:09<00:13, 202MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  41%|████      | 1.87G/4.54G [00:09<00:13, 203MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  42%|████▏     | 1.89G/4.54G [00:09<00:13, 204MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  42%|████▏     | 1.91G/4.54G [00:09<00:12, 204MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  42%|████▏     | 1.93G/4.54G [00:09<00:12, 205MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  43%|████▎     | 1.95G/4.54G [00:09<00:12, 204MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  43%|████▎     | 1.97G/4.54G [00:09<00:12, 205MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  44%|████▍     | 1.99G/4.54G [00:09<00:12, 205MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  44%|████▍     | 2.01G/4.54G [00:10<00:12, 204MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  45%|████▍     | 2.03G/4.54G [00:10<00:12, 204MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  45%|████▌     | 2.06G/4.54G [00:10<00:12, 205MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  46%|████▌     | 2.08G/4.54G [00:10<00:11, 206MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  46%|████▌     | 2.10G/4.54G [00:10<00:11, 206MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  47%|████▋     | 2.12G/4.54G [00:10<00:11, 206MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  47%|████▋     | 2.14G/4.54G [00:10<00:11, 205MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  48%|████▊     | 2.16G/4.54G [00:10<00:11, 205MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  48%|████▊     | 2.18G/4.54G [00:10<00:11, 206MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  48%|████▊     | 2.20G/4.54G [00:10<00:11, 205MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  49%|████▉     | 2.22G/4.54G [00:11<00:11, 205MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  49%|████▉     | 2.24G/4.54G [00:11<00:11, 205MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  50%|████▉     | 2.26G/4.54G [00:11<00:11, 205MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  50%|█████     | 2.29G/4.54G [00:11<00:11, 205MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  51%|█████     | 2.31G/4.54G [00:11<00:10, 205MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  51%|█████▏    | 2.33G/4.54G [00:11<00:10, 205MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  52%|█████▏    | 2.35G/4.54G [00:11<00:10, 205MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  52%|█████▏    | 2.37G/4.54G [00:11<00:10, 205MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  53%|█████▎    | 2.39G/4.54G [00:11<00:10, 205MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  53%|█████▎    | 2.41G/4.54G [00:12<00:10, 205MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  54%|█████▎    | 2.43G/4.54G [00:12<00:10, 203MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  54%|█████▍    | 2.46G/4.54G [00:12<00:09, 208MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  55%|█████▍    | 2.50G/4.54G [00:12<00:09, 211MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  56%|█████▌    | 2.53G/4.54G [00:12<00:09, 212MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  56%|█████▋    | 2.56G/4.54G [00:12<00:09, 213MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  57%|█████▋    | 2.59G/4.54G [00:12<00:09, 213MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  58%|█████▊    | 2.62G/4.54G [00:12<00:08, 214MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  58%|█████▊    | 2.65G/4.54G [00:13<00:08, 213MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  59%|█████▉    | 2.68G/4.54G [00:13<00:08, 213MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  60%|█████▉    | 2.72G/4.54G [00:13<00:08, 213MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  61%|██████    | 2.75G/4.54G [00:13<00:08, 212MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  61%|██████    | 2.78G/4.54G [00:13<00:08, 212MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  62%|██████▏   | 2.81G/4.54G [00:13<00:08, 213MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  63%|██████▎   | 2.84G/4.54G [00:14<00:07, 213MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  63%|██████▎   | 2.87G/4.54G [00:14<00:07, 213MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  64%|██████▍   | 2.90G/4.54G [00:14<00:07, 213MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  65%|██████▍   | 2.94G/4.54G [00:14<00:07, 214MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  65%|██████▌   | 2.97G/4.54G [00:14<00:07, 214MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  66%|██████▌   | 3.00G/4.54G [00:14<00:07, 214MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  67%|██████▋   | 3.03G/4.54G [00:14<00:07, 214MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  67%|██████▋   | 3.06G/4.54G [00:15<00:06, 213MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  68%|██████▊   | 3.09G/4.54G [00:15<00:06, 212MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  69%|██████▉   | 3.12G/4.54G [00:15<00:06, 211MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  70%|██████▉   | 3.16G/4.54G [00:15<00:06, 211MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  70%|███████   | 3.19G/4.54G [00:15<00:06, 211MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  71%|███████   | 3.22G/4.54G [00:15<00:06, 212MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  72%|███████▏  | 3.25G/4.54G [00:15<00:06, 212MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  72%|███████▏  | 3.28G/4.54G [00:16<00:05, 213MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  73%|███████▎  | 3.31G/4.54G [00:16<00:05, 213MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  74%|███████▎  | 3.34G/4.54G [00:16<00:05, 213MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  74%|███████▍  | 3.38G/4.54G [00:16<00:05, 214MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  75%|███████▌  | 3.41G/4.54G [00:16<00:05, 212MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  76%|███████▌  | 3.44G/4.54G [00:16<00:05, 211MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  76%|███████▋  | 3.47G/4.54G [00:16<00:05, 211MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  77%|███████▋  | 3.50G/4.54G [00:17<00:04, 211MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  78%|███████▊  | 3.53G/4.54G [00:17<00:04, 210MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  79%|███████▊  | 3.57G/4.54G [00:17<00:04, 210MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  79%|███████▉  | 3.60G/4.54G [00:17<00:06, 148MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  80%|███████▉  | 3.62G/4.54G [00:17<00:05, 158MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  80%|████████  | 3.64G/4.54G [00:17<00:05, 167MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  81%|████████  | 3.66G/4.54G [00:18<00:04, 176MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  81%|████████  | 3.68G/4.54G [00:18<00:04, 184MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  82%|████████▏ | 3.70G/4.54G [00:18<00:04, 190MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  82%|████████▏ | 3.72G/4.54G [00:18<00:04, 195MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  82%|████████▏ | 3.74G/4.54G [00:18<00:04, 199MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  83%|████████▎ | 3.76G/4.54G [00:18<00:03, 201MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  84%|████████▎ | 3.80G/4.54G [00:18<00:03, 206MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  84%|████████▍ | 3.83G/4.54G [00:18<00:03, 209MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  85%|████████▍ | 3.86G/4.54G [00:19<00:03, 209MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  86%|████████▌ | 3.89G/4.54G [00:19<00:03, 210MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  86%|████████▋ | 3.92G/4.54G [00:19<00:02, 210MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  87%|████████▋ | 3.95G/4.54G [00:19<00:02, 208MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  88%|████████▊ | 3.97G/4.54G [00:19<00:02, 208MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  88%|████████▊ | 4.00G/4.54G [00:19<00:02, 208MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  88%|████████▊ | 4.02G/4.54G [00:19<00:02, 208MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  89%|████████▉ | 4.04G/4.54G [00:19<00:02, 208MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  89%|████████▉ | 4.06G/4.54G [00:20<00:02, 208MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  90%|████████▉ | 4.08G/4.54G [00:20<00:02, 207MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  90%|█████████ | 4.10G/4.54G [00:20<00:02, 207MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  91%|█████████ | 4.12G/4.54G [00:20<00:02, 207MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  91%|█████████ | 4.14G/4.54G [00:20<00:01, 208MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  92%|█████████▏| 4.16G/4.54G [00:20<00:01, 208MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  92%|█████████▏| 4.19G/4.54G [00:20<00:01, 209MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  93%|█████████▎| 4.23G/4.54G [00:20<00:01, 210MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  94%|█████████▍| 4.26G/4.54G [00:20<00:01, 210MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  94%|█████████▍| 4.28G/4.54G [00:21<00:01, 210MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  95%|█████████▍| 4.30G/4.54G [00:21<00:01, 210MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  95%|█████████▌| 4.32G/4.54G [00:21<00:01, 209MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  96%|█████████▌| 4.34G/4.54G [00:21<00:00, 209MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  96%|█████████▌| 4.36G/4.54G [00:21<00:00, 209MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  97%|█████████▋| 4.38G/4.54G [00:21<00:00, 209MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  97%|█████████▋| 4.40G/4.54G [00:21<00:00, 209MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  97%|█████████▋| 4.42G/4.54G [00:21<00:00, 209MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  98%|█████████▊| 4.45G/4.54G [00:21<00:00, 208MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  98%|█████████▊| 4.47G/4.54G [00:21<00:00, 208MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  99%|█████████▉| 4.49G/4.54G [00:22<00:00, 207MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin:  99%|█████████▉| 4.51G/4.54G [00:22<00:00, 207MB/s]\u001b[A\nDownloading (…)l-00002-of-00002.bin: 100%|██████████| 4.54G/4.54G [00:22<00:00, 203MB/s]\u001b[A\nDownloading shards: 100%|██████████| 2/2 [04:15<00:00, 127.95s/it]\nLoading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.53s/it]\nDownloading (…)neration_config.json: 100%|██████████| 120/120 [00:00<00:00, 39.5kB/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[30], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use a pipeline as a high-level helper\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m----> 4\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOpen-Orca/Mistral-7B-OpenOrca\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/pipelines/__init__.py:931\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    928\u001b[0m             tokenizer_kwargs \u001b[38;5;241m=\u001b[39m model_kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    929\u001b[0m             tokenizer_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 931\u001b[0m         tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer_identifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_fast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_image_processor:\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;66;03m# Try to infer image processor from model or config name (if provided as str)\u001b[39;00m\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:751\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    749\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    750\u001b[0m         )\n\u001b[0;32m--> 751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2045\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2043\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2045\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2048\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2256\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2254\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2255\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2256\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   2258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   2259\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2260\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2261\u001b[0m     )\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/models/llama/tokenization_llama_fast.py:122\u001b[0m, in \u001b[0;36mLlamaTokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces, unk_token, bos_token, eos_token, add_bos_token, add_eos_token, use_default_system_prompt, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    111\u001b[0m     vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    121\u001b[0m ):\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_default_system_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_default_system_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_bos_token \u001b[38;5;241m=\u001b[39m add_bos_token\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_eos_token \u001b[38;5;241m=\u001b[39m add_eos_token\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:120\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt instantiate the backend tokenizer from one of: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(1) a `tokenizers` library serialization file, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(2) a slow tokenizer instance to convert or \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(3) an equivalent slow tokenizer class to instantiate and convert. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m     )\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer \u001b[38;5;241m=\u001b[39m fast_tokenizer\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."],"ename":"ValueError","evalue":"Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.","output_type":"error"}]},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"Open-Orca/Mistral-7B-OpenOrca\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Open-Orca/Mistral-7B-OpenOrca\")","metadata":{"execution":{"iopub.status.busy":"2023-10-09T16:22:14.007683Z","iopub.execute_input":"2023-10-09T16:22:14.008854Z","iopub.status.idle":"2023-10-09T16:22:14.178082Z","shell.execute_reply.started":"2023-10-09T16:22:14.008821Z","shell.execute_reply":"2023-10-09T16:22:14.176911Z"},"trusted":true},"execution_count":31,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[31], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load model directly\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOpen-Orca/Mistral-7B-OpenOrca\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpen-Orca/Mistral-7B-OpenOrca\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:751\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    749\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    750\u001b[0m         )\n\u001b[0;32m--> 751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2045\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2043\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2045\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2048\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2256\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2254\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2255\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2256\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   2258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   2259\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2260\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2261\u001b[0m     )\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/models/llama/tokenization_llama_fast.py:122\u001b[0m, in \u001b[0;36mLlamaTokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces, unk_token, bos_token, eos_token, add_bos_token, add_eos_token, use_default_system_prompt, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    111\u001b[0m     vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    121\u001b[0m ):\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_default_system_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_default_system_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_bos_token \u001b[38;5;241m=\u001b[39m add_bos_token\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_eos_token \u001b[38;5;241m=\u001b[39m add_eos_token\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:120\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt instantiate the backend tokenizer from one of: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(1) a `tokenizers` library serialization file, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(2) a slow tokenizer instance to convert or \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(3) an equivalent slow tokenizer class to instantiate and convert. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m     )\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer \u001b[38;5;241m=\u001b[39m fast_tokenizer\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."],"ename":"ValueError","evalue":"Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.","output_type":"error"}]},{"cell_type":"code","source":"# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"Open-Orca/Mistral-7B-OpenOrca\")","metadata":{"execution":{"iopub.status.busy":"2023-10-09T16:22:30.633920Z","iopub.execute_input":"2023-10-09T16:22:30.634387Z","iopub.status.idle":"2023-10-09T16:23:48.471697Z","shell.execute_reply.started":"2023-10-09T16:22:30.634356Z","shell.execute_reply":"2023-10-09T16:23:48.470542Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.86s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[32], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use a pipeline as a high-level helper\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m----> 4\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOpen-Orca/Mistral-7B-OpenOrca\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/pipelines/__init__.py:931\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    928\u001b[0m             tokenizer_kwargs \u001b[38;5;241m=\u001b[39m model_kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    929\u001b[0m             tokenizer_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 931\u001b[0m         tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer_identifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_fast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_image_processor:\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;66;03m# Try to infer image processor from model or config name (if provided as str)\u001b[39;00m\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:751\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    749\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    750\u001b[0m         )\n\u001b[0;32m--> 751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2045\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2043\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2045\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2048\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2256\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2254\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2255\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2256\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   2258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   2259\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2260\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2261\u001b[0m     )\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/models/llama/tokenization_llama_fast.py:122\u001b[0m, in \u001b[0;36mLlamaTokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces, unk_token, bos_token, eos_token, add_bos_token, add_eos_token, use_default_system_prompt, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    111\u001b[0m     vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    121\u001b[0m ):\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_default_system_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_default_system_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_bos_token \u001b[38;5;241m=\u001b[39m add_bos_token\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_eos_token \u001b[38;5;241m=\u001b[39m add_eos_token\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:120\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt instantiate the backend tokenizer from one of: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(1) a `tokenizers` library serialization file, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(2) a slow tokenizer instance to convert or \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(3) an equivalent slow tokenizer class to instantiate and convert. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m     )\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer \u001b[38;5;241m=\u001b[39m fast_tokenizer\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."],"ename":"ValueError","evalue":"Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.","output_type":"error"}]},{"cell_type":"code","source":"# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"gpt2\")","metadata":{"execution":{"iopub.status.busy":"2023-10-09T16:23:54.300784Z","iopub.execute_input":"2023-10-09T16:23:54.301120Z","iopub.status.idle":"2023-10-09T16:23:59.480441Z","shell.execute_reply.started":"2023-10-09T16:23:54.301095Z","shell.execute_reply":"2023-10-09T16:23:59.479575Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"Downloading (…)lve/main/config.json: 100%|██████████| 665/665 [00:00<00:00, 257kB/s]\nDownloading model.safetensors: 100%|██████████| 548M/548M [00:02<00:00, 203MB/s] \nDownloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 49.7kB/s]\nDownloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 21.9MB/s]\nDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 28.6MB/s]\nDownloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 53.1MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")","metadata":{"execution":{"iopub.status.busy":"2023-10-09T16:24:14.473551Z","iopub.execute_input":"2023-10-09T16:24:14.473951Z","iopub.status.idle":"2023-10-09T16:24:16.454877Z","shell.execute_reply.started":"2023-10-09T16:24:14.473921Z","shell.execute_reply":"2023-10-09T16:24:16.453953Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"!pip install transformers\n!pip install torch\n","metadata":{"execution":{"iopub.status.busy":"2023-10-09T16:24:59.792775Z","iopub.execute_input":"2023-10-09T16:24:59.793220Z","iopub.status.idle":"2023-10-09T16:25:10.282436Z","shell.execute_reply.started":"2023-10-09T16:24:59.793171Z","shell.execute_reply":"2023-10-09T16:25:10.281260Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.8/site-packages (4.34.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.8/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/site-packages (from transformers) (4.65.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from transformers) (23.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.8/site-packages (from transformers) (0.17.3)\nRequirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.8/site-packages (from transformers) (0.14.1)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.8/site-packages (from transformers) (0.4.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/site-packages (from transformers) (2023.10.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.9.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (1.26.16)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (2023.5.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (3.2.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nRequirement already satisfied: torch in /usr/local/lib/python3.8/site-packages (2.0.0)\nRequirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.8/site-packages (from torch) (10.2.10.91)\nRequirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.8/site-packages (from torch) (11.7.101)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.8/site-packages (from torch) (3.1.2)\nRequirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.8/site-packages (from torch) (11.7.91)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/site-packages (from torch) (11.7.99)\nRequirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.8/site-packages (from torch) (2.0.0)\nRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.8/site-packages (from torch) (10.9.0.58)\nRequirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.8/site-packages (from torch) (2.14.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.8/site-packages (from torch) (3.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.8/site-packages (from torch) (1.12)\nRequirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.8/site-packages (from torch) (11.7.4.91)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/site-packages (from torch) (4.7.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.8/site-packages (from torch) (3.1)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/site-packages (from torch) (8.5.0.96)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/site-packages (from torch) (11.7.99)\nRequirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.8/site-packages (from torch) (11.4.0.1)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/site-packages (from torch) (11.10.3.66)\nRequirement already satisfied: wheel in /usr/local/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (57.5.0)\nRequirement already satisfied: cmake in /usr/local/lib/python3.8/site-packages (from triton==2.0.0->torch) (3.26.4)\nRequirement already satisfied: lit in /usr/local/lib/python3.8/site-packages (from triton==2.0.0->torch) (16.0.6)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-09T16:25:13.301691Z","iopub.execute_input":"2023-10-09T16:25:13.302156Z","iopub.status.idle":"2023-10-09T16:25:15.661542Z","shell.execute_reply.started":"2023-10-09T16:25:13.302114Z","shell.execute_reply":"2023-10-09T16:25:15.660641Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Load your dataset\ndataset_path = \"/kaggle/input/unplash-dataset/dataset_final.csv\"  \ndf = pd.read_csv(dataset_path)\n\n\nprompts = df[\"photo_description\"] + \" \" + df[\"exif_camera_make\"] + \" \" + df[\"exif_aperture_value\"]\n\n# Convert prompts to a list\nprompt_list = prompts.tolist()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-09T16:26:25.041346Z","iopub.execute_input":"2023-10-09T16:26:25.042325Z","iopub.status.idle":"2023-10-09T16:26:25.250838Z","shell.execute_reply.started":"2023-10-09T16:26:25.042285Z","shell.execute_reply":"2023-10-09T16:26:25.249974Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer\n\n# Load the CSV file into a DataFrame\ncsv_path = \"/kaggle/input/unplash-dataset/dataset_final.csv\"\ndf = pd.read_csv(csv_path)\n\n# Filter out rows with non-string or empty values in the \"photo_description\" column\ndf = df[df[\"photo_description\"].apply(lambda x: isinstance(x, str) and len(x) > 0)]\n\n# Initialize the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\n# Tokenize your dataset\ntokenized_dataset = tokenizer(df[\"photo_description\"].tolist(), truncation=True, padding=True, return_tensors=\"pt\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-09T16:31:08.547694Z","iopub.execute_input":"2023-10-09T16:31:08.548126Z","iopub.status.idle":"2023-10-09T16:31:09.131372Z","shell.execute_reply.started":"2023-10-09T16:31:08.548094Z","shell.execute_reply":"2023-10-09T16:31:09.130119Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"Using pad_token, but it is not set yet.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[43], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Tokenize your dataset\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mphoto_description\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2806\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2805\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2806\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2808\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2892\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2887\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2888\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2889\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2890\u001b[0m         )\n\u001b[1;32m   2891\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 2892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2894\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2909\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2910\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2911\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2913\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2914\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2930\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2931\u001b[0m     )\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3074\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3058\u001b[0m \u001b[38;5;124;03mTokenize and prepare for the model a list of sequences or a list of pairs of sequences.\u001b[39;00m\n\u001b[1;32m   3059\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3070\u001b[0m \u001b[38;5;124;03m        details in `encode_plus`).\u001b[39;00m\n\u001b[1;32m   3071\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3073\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m-> 3074\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_padding_truncation_strategies\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3080\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3081\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3083\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[1;32m   3084\u001b[0m     batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   3085\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3100\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3101\u001b[0m )\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2711\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001b[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2709\u001b[0m \u001b[38;5;66;03m# Test if we have a padding token\u001b[39;00m\n\u001b[1;32m   2710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_strategy \u001b[38;5;241m!=\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_PAD \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m-> 2711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2712\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking to pad but the tokenizer does not have a padding token. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2713\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2714\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor add a new pad token via `tokenizer.add_special_tokens(\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad_token\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m})`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2715\u001b[0m     )\n\u001b[1;32m   2717\u001b[0m \u001b[38;5;66;03m# Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\u001b[39;00m\n\u001b[1;32m   2718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2719\u001b[0m     truncation_strategy \u001b[38;5;241m!=\u001b[39m TruncationStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_TRUNCATE\n\u001b[1;32m   2720\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m padding_strategy \u001b[38;5;241m!=\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_PAD\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2723\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (max_length \u001b[38;5;241m%\u001b[39m pad_to_multiple_of \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   2724\u001b[0m ):\n","\u001b[0;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."],"ename":"ValueError","evalue":"Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.","output_type":"error"}]},{"cell_type":"code","source":"print(df.columns)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-09T16:36:50.585226Z","iopub.execute_input":"2023-10-09T16:36:50.585612Z","iopub.status.idle":"2023-10-09T16:36:50.591042Z","shell.execute_reply.started":"2023-10-09T16:36:50.585585Z","shell.execute_reply":"2023-10-09T16:36:50.590233Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Index(['photo_id', 'photo_image_url', 'photo_width', 'photo_height',\n       'photo_aspect_ratio', 'photo_description', 'exif_camera_make',\n       'exif_camera_model', 'exif_iso', 'exif_aperture_value',\n       'exif_focal_length', 'exif_exposure_time', 'stats_views',\n       'stats_downloads', 'ai_description', 'ai_primary_landmark_name',\n       'ai_primary_landmark_latitude', 'ai_primary_landmark_longitude',\n       'ai_primary_landmark_confidence', 'blur_hash', 'keyword_x',\n       'ai_service_1_confidence', 'ai_service_2_confidence',\n       'conversion_country', 'keyword_y', 'hex', 'red', 'green', 'blue',\n       'keyword', 'ai_coverage', 'ai_score', 'collection_title'],\n      dtype='object')\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\nfrom transformers import AutoModelForSequenceClassification, AdamW\n\n# Prepare labels \nlabels = df[\"labels_column\"].tolist()\n\n# Split data into training and validation sets\ntrain_size = int(0.8 * len(tokenized_dataset))\nval_size = len(tokenized_dataset) - train_size\ntrain_dataset, val_dataset = random_split(tokenized_dataset, [train_size, val_size])\n\n# Create data loaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=16)\n\n# Fine-tuning architecture\nmodel = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=num_classes)\n\n# Define training parameters\noptimizer = AdamW(model.parameters(), lr=1e-5)\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    for batch in train_dataloader:\n        inputs = batch[\"input_ids\"]\n        labels = batch[\"labels\"]\n        optimizer.zero_grad()\n        outputs = model(input_ids=inputs, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\n    # Evaluation on the validation dataset\n    model.eval()\n    with torch.no_grad():\n        for batch in val_dataloader:\n            inputs = batch[\"input_ids\"]\n            labels = batch[\"labels\"]\n            outputs = model(input_ids=inputs, labels=labels)\n            # Calculate and log validation metrics here\n\n# Save the fine-tuned model\nmodel.save_pretrained(\"/path/to/save/model\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-09T16:34:38.582968Z","iopub.execute_input":"2023-10-09T16:34:38.583391Z","iopub.status.idle":"2023-10-09T16:34:42.526649Z","shell.execute_reply.started":"2023-10-09T16:34:38.583359Z","shell.execute_reply":"2023-10-09T16:34:42.525372Z"},"trusted":true},"execution_count":46,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'labels_column'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[46], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForSequenceClassification, AdamW\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Prepare labels (if applicable)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels_column\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Split data into training and validation sets\u001b[39;00m\n\u001b[1;32m      9\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokenized_dataset))\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n","\u001b[0;31mKeyError\u001b[0m: 'labels_column'"],"ename":"KeyError","evalue":"'labels_column'","output_type":"error"}]},{"cell_type":"code","source":"import pandas as pd\nimport random\n\n# Load the CSV file into a DataFrame\ncsv_path = \"/kaggle/input/unplash-dataset/dataset_final.csv\"\ndf = pd.read_csv(csv_path)\n\n# Filter out rows with non-string or empty values in the \"photo_description\" column\ndf = df[df[\"photo_description\"].apply(lambda x: isinstance(x, str) and len(x) > 0)]\n\n\nrandom.seed(42)  \ndf['class_label'] = [random.choice(['positive', 'negative']) for _ in range(len(df))]\n\n# Initialize the tokenizer and set the padding token\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Add a padding token\n\n# Tokenize your dataset\ntokenized_dataset = tokenizer(df[\"photo_description\"].tolist(), truncation=True, padding=True, return_tensors=\"pt\")\n\n# Print the first few rows of the DataFrame with class labels\nprint(df[['photo_description', 'class_label']].head())\n","metadata":{"execution":{"iopub.status.busy":"2023-10-09T16:38:53.551151Z","iopub.execute_input":"2023-10-09T16:38:53.551545Z","iopub.status.idle":"2023-10-09T16:38:55.394488Z","shell.execute_reply.started":"2023-10-09T16:38:53.551517Z","shell.execute_reply":"2023-10-09T16:38:55.393416Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"                             photo_description class_label\n2                 Sunset reflection over river    positive\n3                         Hiking The Mountains    positive\n4  Hiker on snow covered mountains in Similaun    negative\n5                            Sunset over beach    positive\n6                                   Farm Goats    positive\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AutoModelForSequenceClassification, AdamW\n\n\ntrain_size = 0.8\ntrain_dataset, val_dataset = train_test_split(df, train_size=train_size, random_state=42)\n\n\nmodel_name = \"bert-base-uncased\"  \nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Assuming 2 classes (positive and negative)\n\n\nclass TextClassificationDataset(Dataset):\n    def __init__(self, dataset, tokenizer, max_length):\n        self.dataset = dataset\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        text = self.dataset.iloc[idx][\"photo_description\"]\n        label = self.dataset.iloc[idx][\"class_label\"]\n\n        encoding = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\",\n        )\n\n        inputs = {\n            \"input_ids\": encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n            \"labels\": torch.tensor(label, dtype=torch.long),\n        }\n\n        return inputs\n\n\nbatch_size = 32  \nmax_seq_length = 128  \ntrain_data = TextClassificationDataset(train_dataset, tokenizer, max_seq_length)\nval_data = TextClassificationDataset(val_dataset, tokenizer, max_seq_length)\n\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=batch_size)\n\n\noptimizer = AdamW(model.parameters(), lr=1e-5)\nloss_fn = torch.nn.CrossEntropyLoss()  # Assuming a classification task with multiple classes\n\n\nnum_epochs = 5  # You can adjust the number of training epochs\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    for batch in train_loader:\n        inputs = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        outputs = model(**inputs)\n        loss = loss_fn(outputs.logits, inputs[\"labels\"])\n        loss.backward()\n        optimizer.step()\n\n   \n    model.eval()\n    val_losses = []\n    for batch in val_loader:\n        inputs = {k: v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            outputs = model(**inputs)\n            val_loss = loss_fn(outputs.logits, inputs[\"labels\"])\n            val_losses.append(val_loss.item())\n\n    val_loss = sum(val_losses) / len(val_losses)\n    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-09T16:40:17.210819Z","iopub.execute_input":"2023-10-09T16:40:17.211780Z","iopub.status.idle":"2023-10-09T16:40:20.978327Z","shell.execute_reply.started":"2023-10-09T16:40:17.211745Z","shell.execute_reply":"2023-10-09T16:40:20.976944Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stderr","text":"Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 233kB/s]\nDownloading model.safetensors: 100%|██████████| 440M/440M [00:02<00:00, 210MB/s] \nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[49], line 63\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     62\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     64\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     65\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[49], line 38\u001b[0m, in \u001b[0;36mTextClassificationDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     25\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_label\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     27\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[1;32m     28\u001b[0m     text,\n\u001b[1;32m     29\u001b[0m     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     35\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     39\u001b[0m }\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n","\u001b[0;31mTypeError\u001b[0m: new(): invalid data type 'str'"],"ename":"TypeError","evalue":"new(): invalid data type 'str'","output_type":"error"}]},{"cell_type":"code","source":"\n\n\nclass TextClassificationDataset(Dataset):\n    def __init__(self, dataset, tokenizer, max_length):\n        self.dataset = dataset\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        text = self.dataset.iloc[idx][\"photo_description\"]\n        label = self.dataset.iloc[idx][\"class_label\"]\n\n        # Convert label from string to integer\n        label_dict = {\"positive\": 1, \"negative\": 0}\n        label = label_dict[label]\n\n        encoding = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\",\n        )\n\n        inputs = {\n            \"input_ids\": encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n            \"labels\": torch.tensor(label, dtype=torch.long),\n        }\n\n        return inputs\n\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-09T16:41:40.340409Z","iopub.execute_input":"2023-10-09T16:41:40.340768Z","iopub.status.idle":"2023-10-09T16:41:40.348386Z","shell.execute_reply.started":"2023-10-09T16:41:40.340740Z","shell.execute_reply":"2023-10-09T16:41:40.347422Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"\n\n\ntrain_size = int(0.8 * len(tokenized_dataset))\nval_size = len(tokenized_dataset) - train_size\n\ntrain_dataset, val_dataset = random_split(tokenized_dataset, [train_size, val_size])\n\n\nbatch_size = 8\ntrain_loader = DataLoader(\n    TextClassificationDataset(train_dataset, tokenizer, max_length=64),\n    batch_size=batch_size,\n    shuffle=True,\n)\nval_loader = DataLoader(\n    TextClassificationDataset(val_dataset, tokenizer, max_length=64),\n    batch_size=batch_size,\n)\n\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\noptimizer = AdamW(model.parameters(), lr=1e-5)\n\n\nnum_epochs = 5\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    for batch in train_loader:\n        inputs = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n\n        outputs = model(**inputs)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\n    \n    model.eval()\n    val_losses = []\n    val_accuracies = []\n\n    with torch.no_grad():\n        for batch in val_loader:\n            inputs = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**inputs)\n            loss = outputs.loss\n            logits = outputs.logits\n            predictions = torch.argmax(logits, dim=1)\n            labels = inputs[\"labels\"]\n            accuracy = (predictions == labels).float().mean()\n            val_losses.append(loss.item())\n            val_accuracies.append(accuracy.item())\n\n    mean_val_loss = sum(val_losses) / len(val_losses)\n    mean_val_accuracy = sum(val_accuracies) / len(val_accuracies)\n\n    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n    print(f\"  Train Loss: {loss.item():.4f}\")\n    print(f\"  Validation Loss: {mean_val_loss:.4f}\")\n    print(f\"  Validation Accuracy: {mean_val_accuracy:.2%}\")\n\n\nmodel.save_pretrained(\"text_classification_model\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-09T16:42:26.496329Z","iopub.execute_input":"2023-10-09T16:42:26.496686Z","iopub.status.idle":"2023-10-09T16:42:27.506332Z","shell.execute_reply.started":"2023-10-09T16:42:26.496658Z","shell.execute_reply":"2023-10-09T16:42:27.504974Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[54], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     31\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     33\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     34\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[53], line 14\u001b[0m, in \u001b[0;36mTextClassificationDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 14\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m[idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphoto_description\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     15\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_label\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Convert label from string to integer\u001b[39;00m\n","\u001b[0;31mAttributeError\u001b[0m: 'Subset' object has no attribute 'iloc'"],"ename":"AttributeError","evalue":"'Subset' object has no attribute 'iloc'","output_type":"error"}]},{"cell_type":"code","source":"\ntrain_size = int(0.8 * len(tokenized_dataset))\nval_size = len(tokenized_dataset) - train_size\n\ntrain_dataset, val_dataset = random_split(tokenized_dataset, [train_size, val_size])\n\n\nbatch_size = 8\ntrain_loader = DataLoader(\n    TextClassificationDataset(train_dataset, tokenizer, max_length=64),\n    batch_size=batch_size,\n    shuffle=True,\n)\nval_loader = DataLoader(\n    TextClassificationDataset(val_dataset, tokenizer, max_length=64),\n    batch_size=batch_size,\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-09T16:45:28.350899Z","iopub.execute_input":"2023-10-09T16:45:28.351267Z","iopub.status.idle":"2023-10-09T16:45:28.358099Z","shell.execute_reply.started":"2023-10-09T16:45:28.351238Z","shell.execute_reply":"2023-10-09T16:45:28.357149Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\noptimizer = AdamW(model.parameters(), lr=1e-5)\n\n\nnum_epochs = 5\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    for batch in train_loader:\n        inputs = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n\n        outputs = model(**inputs)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\n    \n    model.eval()\n    val_losses = []\n    val_accuracies = []\n\n    with torch.no_grad():\n        for batch in val_loader:\n            inputs = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**inputs)\n            loss = outputs.loss\n            logits = outputs.logits\n            predictions = torch.argmax(logits, dim=1)\n            labels = inputs[\"labels\"]\n            accuracy = (predictions == labels).float().mean()\n            val_losses.append(loss.item())\n            val_accuracies.append(accuracy.item())\n\n    mean_val_loss = sum(val_losses) / len(val_losses)\n    mean_val_accuracy = sum(val_accuracies) / len(val_accuracies)\n\n    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n    print(f\"  Train Loss: {loss.item():.4f}\")\n    print(f\"  Validation Loss: {mean_val_loss:.4f}\")\n    print(f\"  Validation Accuracy: {mean_val_accuracy:.2%}\")\n\n\nmodel.save_pretrained(\"text_classification_model\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-09T16:46:06.000469Z","iopub.execute_input":"2023-10-09T16:46:06.000902Z","iopub.status.idle":"2023-10-09T16:46:07.023733Z","shell.execute_reply.started":"2023-10-09T16:46:06.000870Z","shell.execute_reply":"2023-10-09T16:46:07.022644Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[59], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     13\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     15\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     16\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[53], line 14\u001b[0m, in \u001b[0;36mTextClassificationDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 14\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m[idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphoto_description\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     15\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_label\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Convert label from string to integer\u001b[39;00m\n","\u001b[0;31mAttributeError\u001b[0m: 'Subset' object has no attribute 'iloc'"],"ename":"AttributeError","evalue":"'Subset' object has no attribute 'iloc'","output_type":"error"}]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\nfrom torch.utils.data import DataLoader, Dataset, random_split\n\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Add a padding token\n\nclass TextClassificationDataset(Dataset):\n    def __init__(self, dataset, tokenizer, max_length):\n        self.dataset = dataset\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        text = self.dataset.iloc[idx][\"photo_description\"]\n        label = self.dataset.iloc[idx][\"class_label\"]\n        \n        encoding = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            max_length=self.max_length,\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        \n        inputs = {\n            \"input_ids\": encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n            \"labels\": torch.tensor(label, dtype=torch.long)\n        }\n        \n        return inputs\n\n\ntrain_size = int(0.8 * len(df))\nval_size = len(df) - train_size\n\ntrain_dataset, val_dataset = random_split(df, [train_size, val_size])\n\nbatch_size = 8\ntrain_loader = DataLoader(\n    TextClassificationDataset(train_dataset, tokenizer, max_length=64),\n    batch_size=batch_size,\n    shuffle=True,\n)\n\nval_loader = DataLoader(\n    TextClassificationDataset(val_dataset, tokenizer, max_length=64),\n    batch_size=batch_size,\n)\n\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\noptimizer = AdamW(model.parameters(), lr=1e-5)\n\nnum_epochs = 5\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    for batch in train_loader:\n        inputs = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n\n        outputs = model(**inputs)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\n    model.eval()\n    val_losses = []\n    val_accuracies = []\n\n    with torch.no_grad():\n        for batch in val_loader:\n            inputs = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**inputs)\n            loss = outputs.loss\n            logits = outputs.logits\n            predictions = torch.argmax(logits, dim=1)\n            labels = inputs[\"labels\"]\n            accuracy = (predictions == labels).float().mean()\n            val_losses.append(loss.item())\n            val_accuracies.append(accuracy.item())\n\n    mean_val_loss = sum(val_losses) / len(val_losses)\n    mean_val_accuracy = sum(val_accuracies) / len(val_accuracies)\n\n    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n    print(f\"  Train Loss: {loss.item():.4f}\")\n    print(f\"  Validation Loss: {mean_val_loss:.4f}\")\n    print(f\"  Validation Accuracy: {mean_val_accuracy:.2%}\")\n\n\nmodel.save_pretrained(\"text_classification_model\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-09T16:47:44.400673Z","iopub.execute_input":"2023-10-09T16:47:44.401108Z","iopub.status.idle":"2023-10-09T16:47:45.821671Z","shell.execute_reply.started":"2023-10-09T16:47:44.401075Z","shell.execute_reply":"2023-10-09T16:47:45.820525Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stderr","text":"Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 11.7kB/s]\nDownloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 6.74MB/s]\nDownloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 15.2MB/s]\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[61], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     72\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     74\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     75\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[61], line 24\u001b[0m, in \u001b[0;36mTextClassificationDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 24\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m[idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphoto_description\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     25\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_label\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     27\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[1;32m     28\u001b[0m         text,\n\u001b[1;32m     29\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m     )\n","\u001b[0;31mAttributeError\u001b[0m: 'Subset' object has no attribute 'iloc'"],"ename":"AttributeError","evalue":"'Subset' object has no attribute 'iloc'","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}